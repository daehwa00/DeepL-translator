# 코드
제가 구현한 SIFT의 Python 코드는 여기에서 찾을 수 있습니다. 이 튜토리얼에서는 이 코드(파일 pysift.py)를 단계별로 살펴보면서 매 순간 무슨 일이 일어나는지 완전히 이해할 수 있도록 변수를 인쇄하고 시각화해 보겠습니다. 저는 이 구현을 OpenCV 구현을 면밀히 따르면서 로직을 단순화하고 파이썬화하여 세부 사항을 희생하지 않고 작성했습니다.

사용법은 간단합니다:

```
import cv2
import pysift

image = cv2.imread('your_image.png', 0)
키포인트, 설명자 = pysift.computeKeypointsAndDescriptors(이미지)
```

2D NumPy 배열을 `computKeypointsAndDescriptors()`에 전달하기만 하면 OpenCV `KeyPoint` 객체 목록과 연결된 128 길이의 설명자 벡터 목록을 반환합니다. 이렇게 하면 `pysift`가 OpenCV의 SIFT 기능을 대체하는 드롭인 방식으로 작동합니다. 이 구현은 명확하고 이해하기 쉽게 하기 위한 것이며, 고성능을 위해 설계된 것은 아닙니다. 일반 노트북에서 일반적인 입력 이미지를 처리하는 데 몇 분 정도 걸립니다.

리포지토리를 복제하고 템플릿 매칭 데모를 사용해 보세요. OpenCV를 사용할 때와 거의 동일한 키포인트를 얻을 수 있습니다(차이는 부동 소수점 오류로 인한 것입니다).

# SIFT 이론 및 개요
SIFT의 이론적 배경을 간단히 살펴보고 알고리즘의 개략적인 로드맵을 만들어 보겠습니다. 수학적인 내용은 다루지 않겠습니다. 원본 논문은 [여기](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)에서 읽어보실 수 있으며, 반드시 읽어보셔야 합니다.

SIFT는 이미지의 너비, 높이, 그리고 가장 중요한 축척에 걸쳐 특징적인 키포인트를 식별합니다. 스케일을 고려하면 관심 있는 템플릿의 크기가 변경되거나 이미지 품질이 좋아지거나 나빠지거나 템플릿의 시점이나 종횡비가 변경되더라도 어느 정도 안정적으로 유지되는 키포인트를 식별할 수 있습니다. 또한 각 키포인트에는 템플릿 회전과 무관하게 SIFT 기능이 작동하도록 하는 관련 방향이 있습니다. 마지막으로, SIFT는 각 키포인트에 대한 설명자(키포인트를 비교할 수 있는 128 길이의 벡터)를 생성합니다. 이러한 설명자는 키포인트의 주변에서 계산된 그라데이션의 히스토그램에 불과합니다.

SIFT의 까다로운 세부 사항은 대부분 입력 이미지에 정확한 양의 블러를 적용하거나 키포인트를 한 축척에서 다른 축척으로 변환하는 등 축척 공간과 관련이 있습니다.

아래에서 `pysift`의 주요 함수인 `computeKeypointsAndDescriptors()`를 보면 SIFT에 관련된 다양한 구성 요소를 명확하게 파악할 수 있습니다. 먼저 `generateBaseImage()`를 호출하여 입력 이미지를 적절히 흐리게 하고 두 배로 늘려 스케일 공간을 형성하는 연속적으로 흐리게 하고 다운샘플링한 이미지 집합인 "이미지 피라미드"의 기본 이미지를 생성합니다. 그런 다음 `computeNumberOfOctaves()`를 호출하여 이미지 피라미드의 레이어 수("옥타브")를 계산합니다. 이제 이미지 피라미드를 실제로 만들 수 있습니다. 가우스 커널 크기(가우스 커널 크기)의 목록을 생성하기 위해 `generateGaussianKernels()`로 시작하여 기본 이미지를 반복적으로 흐리게 하고 다운샘플링하는 `generateGaussianImages()`로 전달합니다. 다음으로 인접한 가우시안 이미지 쌍을 빼서 가우시안 차이("DoG") 이미지의 피라미드를 형성합니다. 이 최종 DoG 이미지 피라미드를 사용하여 `findScaleSpaceExtrema()`를 사용하여 키포인트를 식별합니다. 중복을 제거하고 입력 이미지 크기로 변환하여 이러한 키포인트를 정리하겠습니다. 마지막으로 `generateDescriptors()`를 통해 각 키포인트에 대한 설명자를 생성합니다.

```
import all, any, array, arctan2, cos, sin, exp, dot, log, logical_and, roll, sqrt, stack, trace, unravel_index, pi, deg2rad, rad2deg, where, zeros, floor, full, nan, isnan, round, float32 from numpy.
numpy.linalg에서 det, lstsq, norm을 가져옵니다.
cv2에서 크기 조정, 가우시안 블러, 빼기, 키포인트, inter_linear, inter_nearest를 가져옵니다.
함수 도구에서 cmp_to_key를 가져옵니다.
import logging

####################
전역 변수 ## 전역 변수
####################

logger = logging.getLogger(__name__)
float_tolerance = 1e-7

#################
주요 함수 ## 메인 함수
#################

def computeKeypointsAndDescriptors(이미지, 시그마=1.6, 넘버_간격=3, 가정된_흐림=0.5, 이미지_경계_폭=5):
    """입력 이미지에 대한 SIFT 키포인트와 디스크립터를 계산합니다.
    """
    image = image.astype('float32')
    base_image = generateBaseImage(image, sigma, assumed_blur)
    num_octaves = computeNumberOfOctaves(base_image.shape)
    가우시안 커널 = generateGaussianKernels(sigma, num_intervals)
    가우시안_이미지 = generateGaussianImages(base_image, num_octaves, gaussian_kernels)
    dog_images = generateDoGImages(가우시안_이미지)
    키포인트 = findScaleSpaceExtrema(가우시안_이미지, dog_이미지, num_intervals, 시그마, image_border_width)
    키포인트 = 제거중복키포인트(키포인트)
    키포인트 = convertKeypointsToInputImageSize(키포인트)
    설명자 = 생성 설명자(키포인트, 가우시안_이미지)
    반환 키포인트, 설명자
```

충분히 간단합니다. 이제 이러한 함수를 한 번에 하나씩 살펴보겠습니다.

# 스케일 스페이스 및 이미지 피라미드
```
def generateBaseImage(image, sigma, assumed_blur):
    """입력 이미지에서 양방향으로 2씩 업샘플링하고 흐리게 처리하여 베이스 이미지를 생성합니다.
    """
    logger.debug('기본 이미지 생성 중...')
    image = resize(image, (0, 0), fx=2, fy=2, interpolation=INTER_LINEAR)
    sigma_diff = sqrt(max((sigma ** 2) - ((2 * assumed_blur) ** 2), 0.01))
    return GaussianBlur(image, (0, 0), sigmaX=sigma_diff, sigmaY=sigma_diff)  # 이제 이미지 블러는 가정된_블러 대신 시그마가 됩니다.

def computeNumberOfOctaves(image_shape):
    """기본 이미지 모양에 따라 이미지 피라미드에서 옥타브 수를 계산합니다(OpenCV 기본값).
    """
    반환 int(round(log(min(image_shape))) / log(2) - 1))

def generateGaussianKernels(sigma, num_intervals):
    """입력 이미지를 흐리게 할 가우시안 커널 목록을 생성합니다. 시그마, 간격, 옥타브의 기본값은 Lowe의 논문 섹션 3을 따릅니다.
    """
    logger.debug('스케일 생성 중...')
    num_images_per_옥타브 = num_intervals + 3
    k = 2 ** (1. / num_intervals)
    가우시안 커널 = 0(num_images_per_octave) # 한 옥타브 내에서 한 블러 스케일에서 다음 블러 스케일로 이동하는 데 필요한 가우시안 블러 스케일
    가우시안 커널[0] = 시그마

    범위(1, num_images_per_옥타브)의 image_index에 대해:
        sigma_previous = (k ** (image_index - 1)) * sigma
        시그마_총계 = k * 시그마_이전
        가우스 커널[이미지_인덱스] = sqrt(sigma_total ** 2 - sigma_previous ** 2)
    반환 가우시안 커널

def generateGaussianImages(image, num_octaves, gaussian_kernels):
    """가우시안 이미지의 스케일-공간 피라미드를 생성합니다.
    """
    logger.debug('가우시안 이미지 생성 중...')
    가우시안 이미지 = []

    범위(num_octaves)의 옥타브_인덱스에 대해:
        가우시안_이미지_in_옥타브 = []
        gaussian_images_in_octave.append(image) # 옥타브의 첫 번째 이미지에 이미 올바른 블러가 있습니다.
        가우시안_커널의 가우시안_커널[1:]:
            image = GaussianBlur(image, (0, 0), sigmaX=gaussian_kernel, sigmaY=gaussian_kernel)
            가우시안_이미지_in_옥타브.append(이미지)
        가우시안_이미지.append(가우시안_이미지_in_옥타브)
        옥타브_베이스 = 가우시안_이미지_인_옥타브[-3]
        image = resize(octave_base, (int(octave_base.shape[1] / 2), int(octave_base.shape[0] / 2)), interpolation=INTER_NEAREST)
    반환 배열(가우시안_이미지)

def generateDoGImages(가우시안_이미지):
    """가우시안 차분 이미지 피라미드 생성
    """
    logger.debug('가우시안 차 이미지 생성 중...')
    dog_images = []

    가우시안 이미지의 가우시안_이미지_in_옥타브에 대해:
        도그_이미지_in_옥타브 = []
        FOR FIRST_IMAGE, SECOND_IMAGE IN ZIP(GAUSIAN_IMAGES_IN_옥타브, GAUSIAN_IMAGES_IN_옥타브[1:]):
            dog_images_in_octave.append(subtract(second_image, first_image))  # 이미지가 부호가 없는 정수이므로 일반 뺄셈은 작동하지 않습니다.
        dog_images.append(dog_images_in_octave)
    반환 array(dog_images)
```

첫 번째 단계는 입력 이미지의 크기를 두 배로 늘리고 가우시안 블러를 적용하는 `generateBaseImage()`입니다. 입력 이미지의 블러가 `assumed_blur = 0.5`라고 가정할 때, 결과 베이스 이미지의 블러를 `sigma`로 만들려면 두 배가 된 입력 이미지에 `sigma_diff`만큼 블러를 적용해야 합니다. 입력 이미지를 커널 크기 $σ_1$만큼 흐리게 한 다음 결과 이미지를 $σ_2$만큼 흐리게 하는 것은 $σ^2 = σ_1^2 + σ_2^2$에서 입력 이미지를 $σ$만큼 한 번만 흐리게 하는 것과 동일하다는 점에 유의하세요. (관심 있는 독자는 여기에서 증명을 찾을 수 있습니다.)

이제 간단하지만 약간의 설명이 필요한 `computeNumberOfOctaves()`를 살펴보겠습니다. 이 함수는 이미지가 너무 작아질 때까지 반복적으로 이미지를 반으로 줄일 수 있는 횟수를 계산합니다. 우선, 최종 이미지의 측면 길이는 1픽셀 이상이어야 합니다. 이에 대한 방정식을 설정할 수 있습니다. y$가 이미지의 짧은 측면 길이라면 $y / 2^x = 1$이 되고, 여기서 $x$는 기본 이미지를 절반으로 줄일 수 있는 횟수입니다. 양변의 로그를 취하고 $x$를 풀면 $\log(y) / \log(2)$를 구할 수 있습니다. 그렇다면 위의 함수에서 왜 $-1$이 나타날까요? 결국 이미지 피라미드에 정수의 레이어 수를 가지려면 $x$를 가장 가까운 정수(floor$(x)$로 반내림해야 합니다.

실제로 아래 함수에서 `numOctaves`가 어떻게 사용되는지 살펴보면, 기본 이미지 `numOctaves - 1`을 반으로 줄여 기본 이미지를 포함한 `numOctaves` 레이어로 끝납니다. 이렇게 하면 가장 높은 옥타브(가장 작은 이미지)의 이미지의 측면 길이가 최소 3이 됩니다. 이는 나중에 각 DoG 이미지에서 최소값과 최대값을 검색할 때 3×3픽셀 이웃을 고려해야 하기 때문에 중요합니다.

다음으로 특정 레이어에 있는 각 이미지의 블러 양 목록을 생성하는 `generateGaussianKernels()`를 호출합니다. 이미지 피라미드에는 `numOctaves` 레이어가 있지만 각 레이어 자체에는 `numIntervals + 3` 이미지가 있다는 점에 유의하세요. 같은 레이어에 있는 모든 이미지의 너비와 높이는 동일하지만 흐림의 양은 연속적으로 증가합니다. '+ 3'은 어디에서 오는 것일까요? 하나의 흐림 값에서 그 값의 두 배까지 `numIntervals` 단계를 커버하기 위해 `numIntervals + 1` 이미지가 있습니다. 레이어의 첫 번째 이미지 앞에 하나의 블러 단계와 레이어의 마지막 이미지 뒤에 또 다른 블러 단계를 위한 또 다른 `+ 2`가 있습니다. 마지막에 이 두 개의 추가 이미지가 필요한 이유는 인접한 가우시안 이미지를 빼서 DoG 이미지 피라미드를 생성하기 때문입니다. 즉, 인접한 두 레이어의 이미지를 비교하면 동일한 블러 값이 많이 반복되는 것을 볼 수 있습니다. 이미지를 뺄 때 모든 블러 단계를 커버하려면 이러한 반복이 필요합니다.

잠시 멈추고 생성된 커널을 인쇄해 보겠습니다:

```
print(가우시안_커널)
array([1.6, 1.22627, 1.54501, 1.94659, 2.45255, 3.09002])
```

이상하네요 - 어떻게 `1.6`에서 `1.22627`로 떨어졌다가 다시 증가하나요? 생성 가우시안 커널()` 함수를 다시 한 번 잘 살펴봅시다. 이 배열의 첫 번째 요소는 단순히 시작 `시그마`이지만, 그 이후 각 요소는 이전 스케일로 컨볼브하는 데 필요한 추가 스케일입니다. 구체적으로 설명하자면 배율이 `1.6`인 이미지로 시작합니다. 이 이미지를 `1.22627`의 가우스 커널로 블러링하여 `sqrt(1.6 ** 2 + 1.22627 ** 2) == 2.01587`의 블러 이미지를 생성하고, 이 새 이미지를 `1.54501` 크기의 커널로 블러링하여 `sqrt(2.01587 ** 2 + 1.54501 ** 2) == 2.53984`의 세 번째 블러 이미지를 생성합니다. 마지막으로 이 이미지를 `1.94659`만큼 흐리게 하여 마지막 이미지를 생성하면 `sqrt(2.53984 ** 2 + 1.94659 ** 2) == 3.2`의 흐리게 처리된 이미지가 생성됩니다. 하지만 `2 * 1.6 == 3.2`이므로 정확히 한 옥타브 올라간 것입니다!

이제 이미지 피라미드를 실제로 생성하는 데 필요한 모든 것이 준비되었습니다. 기본 이미지로 시작하여 '가우시안_커널'에 따라 연속적으로 흐리게 처리하는 방식으로 `GaussianImages()`를 생성합니다. 이미 해당 블러 값이 있는 이미지로 시작하기 때문에 `gaussian_kernels`의 첫 번째 요소는 건너뜁니다. 세 번째에서 마지막 이미지에는 원하는 적절한 블러가 있으므로 이를 반으로 줄이고 다음 레이어를 시작할 때 사용합니다. 이렇게 하면 앞서 언급한 멋진 겹침 효과를 얻을 수 있습니다. 마지막으로 가우시안 블러 처리된 이미지의 연속적인 쌍을 빼서 `generateDoGImages()`를 호출합니다. 주의 - 여기서는 입력 이미지를 `float32`로 캐스팅했기 때문에 일반적인 뺄셈이 작동하지만, 이 캐스팅을 제거하고 `uint` 유형 이미지를 전달해도 코드가 손상되지 않도록 OpenCV의 `subtract()` 함수를 사용할 것입니다.

아래에는 가우스 피라미드의 세 번째 레이어인 `gaussian_images[2]`를 플롯했습니다. 이미지가 점점 더 부드러워지고 미세한 특징이 사라지는 것을 볼 수 있습니다. 또한 `dog_images[2]`를 플롯했습니다. dog_images[2][i] = 가우시안 이미지[2][i + 1] - 가우시안 이미지[2][i]`를 기억하고, DoG 이미지가 어떻게 에지 맵처럼 보이는지 확인합니다.

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_01.png" width="840px"></p>

<center>가우시안 이미지 피라미드의 세 번째 레이어에서 가져온 이미지입니다.</center>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_02.png" width="840px"></p>

<center>가우시안 차이 이미지 피라미드의 세 번째 레이어에서 가져온 이미지입니다.</center>

코드를 실행하고 이 이미지를 직접 플롯하여 비슷한 결과가 나오는지 확인해 보세요.

드디어 DoG 이미지 피라미드가 완성되었습니다. 이제 키포인트를 찾을 차례입니다.

# 스케일 공간 극한점 찾기
```
def findScaleSpaceExtrema(가우시안_이미지, 도그_이미지, num_intervals, 시그마, 이미지_경계_폭, 대비_임계값=0.04):
    """이미지 피라미드에서 모든 스케일-공간 극값의 픽셀 위치를 찾습니다.
    """
    logger.debug('스케일-공간 극한값 찾기...')
    threshold = floor(0.5 * contrast_threshold / num_intervals * 255) # OpenCV 구현에서 가져옴
    키포인트 = []

    for 옥타브_인덱스, 도그이미지_인_옥타브 in enumerate(dog_images):
        for image_index, (first_image, second_image, third_image) in enumerate(zip(dog_images_in_octave, dog_images_in_octave[1:], dog_images_in_octave[2:])):
            # (i, j)는 3x3 배열의 중심입니다.
            for i in range(image_border_width, first_image.shape[0] - image_border_width):
                for j in range(image_border_width, first_image.shape[1] - image_border_width)::
                    if isPixelAnExtremum(first_image[i-1:i+2, j-1:j+2], second_image[i-1:i+2, j-1:j+2], third_image[i-1:i+2, j-1:j+2], threshold):
                        localization_result = localizeExtremumViaQuadraticFit(i, j, image_index + 1, octave_index, num_intervals, dog_images_in_octave, sigma, contrast_threshold, image_border_width)
                        현지화_결과가 None이 아닌 경우:
                            키포인트, 현지화_이미지_색인 = 현지화_결과
                            키포인트_with_방향 = computeKeypointsWithOrientations(키포인트, 옥타브_인덱스, 가우시안_이미지[옥타브_인덱스][현지화된_이미지_인덱스])
                            키포인트_위드_오리엔테이션의 키포인트_위드_오리엔테이션에 대해:
                                keypoints.append(keypoint_with_orientation)
    키포인트 반환

def isPixelAnExtremum(first_subimage, second_subimage, third_subimage, threshold):
    """3x3x3 입력 배열의 중심 요소가 모든 이웃 요소보다 엄격하게 크거나 작으면 True를 반환하고, 그렇지 않으면 False를 반환합니다.
    """
    center_pixel_value = second_subimage[1, 1]
    if abs(center_pixel_value) > 임계값:
        if center_pixel_value > 0:
            return all(center_pixel_value >= first_subimage) 및 \.
                   all(center_pixel_value >= third_subimage) 및 \.
                   all(center_pixel_value >= second_subimage[0, :]) 및 \.
                   all(center_pixel_value >= second_subimage[2, :]) 및 \.
                   center_pixel_value >= second_subimage[1, 0] 및 \.
                   center_pixel_value >= second_subimage[1, 2]
        elif center_pixel_value < 0:
            return all(center_pixel_value <= first_subimage) 및 \.
                   all(center_pixel_value <= third_subimage) 및 \.
                   all(center_pixel_value <= second_subimage[0, :]) 및 \.
                   all(center_pixel_value <= second_subimage[2, :]) 및 \.
                   center_pixel_value <= second_subimage[1, 0] 및 \.
                   center_pixel_value <= second_subimage[1, 2]
    반환 False
```

이 부분은 쉽습니다. 한 번에 세 개의 이미지를 연속적으로 촬영하면서 각 레이어를 반복하기만 하면 됩니다. 레이어의 모든 이미지는 크기가 같고 흐림의 양만 다르다는 점을 기억하세요. 각 세 개의 이미지에서 가운데 이미지의 이웃 8개, 아래 이미지의 이웃 9개, 위 이미지의 이웃 9개 등 26개의 이웃 이미지 모두보다 크거나 작은 픽셀을 찾습니다. 이 검사는 `isPixelAnExtremum()` 함수가 수행합니다. 이것이 최대값과 최소값입니다(엄밀히 말하면 모든 이웃 픽셀과 값이 같은 픽셀을 포함하므로 새들 포인트가 포함됩니다). 극값을 찾으면 아래에서 자세히 설명하는 `localizeExtremumViaQuadraticFit()`을 사용하여 세 가지 차원(폭, 높이, 배율)을 따라 서브픽셀 수준에서 위치를 찾습니다.

마지막 단계는 각 키포인트의 방향을 계산하는 것으로, 이 튜토리얼의 2부에서 다룰 것입니다. 방향이 두 개 이상일 수 있으므로 각 방향에 대해 새 키포인트를 생성하고 추가합니다.

현지화 후에는 키포인트의 새 픽셀 위치에 충분한 대비가 있는지 확인합니다. 이에 대한 자세한 설명은 SIFT 백서에 나와 있으므로 수식은 생략하겠습니다. 간단히 말해, 키포인트 이미지의 폭과 높이에 따른 2D 헤시안 고유값의 비율은 대비 정보를 제공합니다.

마지막으로 키포인트의 위치(`keypoint.pt`)는 레이어에 따라 반복적으로 두 배가 되어 기본 이미지의 좌표와 일치하도록 합니다. '키포인트.옥타브' 및 '키포인트.크기'(즉, 스케일) 속성은 OpenCV 구현에 따라 정의됩니다. 이 두 가지는 나중에 유용하게 사용될 것입니다.

기본 이미지 위에 키포인트를 플로팅하여 SIFT가 흥미롭다고 생각하는 픽셀을 시각화해 볼 수 있습니다. 아래 리포지토리의 box.png에서 그렇게 해보았습니다:

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_03.png" width="600px"></p>

<center>기본 이미지 위에 그려진 초기 키포인트 </center>

# 극한값 현지화하기
```
def localizeExtremumViaQuadraticFit(i, j, image_index, octave_index, num_intervals, dog_images_in_octave, sigma, contrast_threshold, image_border_width, eigenvalue_ratio=10, num_attempts_until_convergence=5):
    """각 극값의 이웃을 중심으로 이차 적합을 통해 스케일 공간 극값의 픽셀 위치를 반복적으로 다듬습니다.
    """
    logger.debug('스케일-공간 극한값 현지화...')
    extremum_is_outside_image = False
    image_shape = dog_images_in_octave[0].shape
    시도_인덱스에 대해 범위(num_attempts_until_convergence):
        # 파생값을 계산하기 위해 uint8에서 float32로 변환해야 하며 Lowe의 임계값을 적용하려면 픽셀 값을 [0, 1]로 리스케일해야 합니다.
        first_image, second_image, third_image = dog_images_in_octave[image_index-1:image_index+2]
        픽셀 큐브 = 스택([first_image[i-1:i+2, j-1:j+2],
                            second_image[i-1:i+2, j-1:j+2],
                            third_image[i-1:i+2, j-1:j+2]]).astype('float32') / 255.
        그라디언트 = 중심 픽셀에서 그라디언트 계산(픽셀_큐브)
        헤시안 = 계산헤시안앳센터픽셀(픽셀_큐브)
        extremum_update = -lstsq(hessian, gradient, rcond=None)[0]
        abs(extremum_update[0]) < 0.5 및 abs(extremum_update[1]) < 0.5 및 abs(extremum_update[2]) < 0.5:
            break
        j += int(round(extremum_update[0]))
        i += int(round(extremum_update[1]))
        image_index += int(round(extremum_update[2]))
        # 새 픽셀 큐브가 이미지 안에 완전히 위치하는지 확인합니다.
        if i < image_border_width 또는 i >= image_shape[0] - image_border_width 또는 j < image_border_width 또는 j >= image_shape[1] - image_border_width 또는 image_index < 1 또는 image_index > num_intervals:
            extremum_is_outside_image = True
            break
    극단값이 이미지 외부에 있는 경우:
        logger.debug('업데이트된 극값이 수렴에 도달하기 전에 이미지 외부로 이동했습니다. 스키핑...')
        반환 없음
    if attempt_index >= num_attempts_until_convergence - 1:
        logger.debug('이 극한값에 대한 수렴에 도달하지 않고 최대 시도 횟수를 초과했습니다. Skipping...')
        반환 없음
    functionValueAtUpdatedExtremum = pixel_cube[1, 1, 1] + 0.5 * dot(gradient, extremum_update)
    abs(functionValueAtUpdatedExtremum) * num_intervals >= contrast_threshold:
        xy_hessian = hessian[:2, :2]
        xy_hessian_trace = trace(xy_hessian)
        xy_hessian_det = det(xy_hessian)
        만약 xy_hessian_det > 0이고 고유값_비율 * (xy_hessian_trace ** 2) < ((고유값_비율 + 1) ** 2) * xy_hessian_det:
            # 대비 검사 통과 -- OpenCV KeyPoint 객체를 생성하고 반환합니다.
            keypoint = KeyPoint()
            keypoint.pt = ((j + extremum_update[0]) * (2 ** 옥타브_인덱스), (i + extremum_update[1]) * (2 ** 옥타브_인덱스))
            keypoint.octave = octave_index + image_index * (2 ** 8) + int(round((extremum_update[2] + 0.5) * 255)) * (2 ** 16)
            keypoint.size = 시그마 * (2 ** ((image_index + extremum_update[2]) / float32(num_intervals)))) * (2 ** (옥타브_인덱스 + 1))  입력 이미지가 두 배가 되었기 때문에 # 옥타브_인덱스 + 1
            keypoint.response = abs(functionValueAtUpdatedExtremum)
            반환 키포인트, 이미지_인덱스
    반환 없음

def computeGradientAtCenterPixel(pixel_array):
    """3x3x3 배열의 중심 픽셀 [1, 1, 1]에서 O(h^2) 차수의 중심 차등 공식을 사용하여 대략적인 그라데이션, 여기서 h는 단계 크기입니다.
    """
    # 스텝 크기 h의 경우, f'(x)에 대한 O(h^2) 차수의 중심 차분 공식은 (f(x + h) - f(x - h)) / (2 * h)입니다.
    # 여기서 h = 1이므로 공식은 f'(x) = (f(x + 1) - f(x - 1)) / 2로 단순화됩니다.
    # 참고: x는 두 번째 배열 축에 해당하고, y는 첫 번째 배열 축에 해당하며, s(스케일)는 세 번째 배열 축에 해당합니다.
    dx = 0.5 * (픽셀 배열[1, 1, 2] - 픽셀 배열[1, 1, 0])
    dy = 0.5 * (픽셀 배열[1, 2, 1] - 픽셀 배열[1, 0, 1])
    ds = 0.5 * (pixel_array[2, 1, 1] - pixel_array[0, 1, 1])
    return array([dx, dy, ds])

def computeHessianAtCenterPixel(pixel_array):
    """3x3x3 배열의 중심 픽셀 [1, 1, 1]에서 O(h^2) 차수의 중심 차분 공식을 사용하여 헤시안 근사값을 구합니다. 여기서 h는 단계 크기입니다.
    """
    # 스텝 크기 h의 경우, f''(x)에 대한 O(h^2) 차수의 중심 차분 공식은 (f(x + h) - 2 * f(x) + f(x - h)) / (h ^ 2) 입니다.
    # 여기서 h = 1이므로 공식은 f''(x) = f(x + 1) - 2 * f(x) + f(x - 1)로 단순화됩니다.
    # 스텝 크기 h의 경우, (d^2) f(x, y) / (dx dy)에 대한 O(h^2) 차수의 중심 차분 공식은 (f(x + h, y + h) - f(x + h, y - h) - f(x - h, y + h) + f(x - h, y - h)) / (4 * h ^ 2) 입니다.
    # 여기서 h = 1이므로 공식은 (d^2) f(x, y) / (dx dy) = (f(x + 1, y + 1) - f(x + 1, y - 1) - f(x - 1, y + 1) + f(x - 1, y - 1)) / 4로 단순화됩니다.
    # 참고: x는 두 번째 배열 축, y는 첫 번째 배열 축, s(스케일)는 세 번째 배열 축에 해당합니다.
    center_pixel_value = pixel_array[1, 1, 1]
    dxx = pixel_array[1, 1, 2] - 2 * center_pixel_value + pixel_array[1, 1, 0]
    dyy = pixel_array[1, 2, 1] - 2 * center_pixel_value + pixel_array[1, 0, 1]
    dss = pixel_array[2, 1, 1] - 2 * center_pixel_value + pixel_array[0, 1, 1]
    dxy = 0.25 * (pixel_array[1, 2, 2] - pixel_array[1, 2, 0] - pixel_array[1, 0, 2] + pixel_array[1, 0, 0])
    dxs = 0.25 * (pixel_array[2, 1, 2] - pixel_array[2, 1, 0] - pixel_array[0, 1, 2] + pixel_array[0, 1, 0])
    dys = 0.25 * (pixel_array[2, 2, 1] - pixel_array[2, 0, 1] - pixel_array[0, 2, 1] + pixel_array[0, 0, 1])
    return array([[dxx, dxy, dxs],
                  [dxy, dyy, dys],
                  [dxs, dys, dss]]))
```

키포인트를 지역화하는 코드는 복잡해 보이지만 실제로는 매우 간단합니다. 이 코드는 원본 SIFT 문서에 설명된 현지화 절차를 그대로 구현합니다. 입력 키포인트 픽셀과 그 주변 픽셀 26개(이를 '픽셀_큐브'라고 부릅니다)에 이차 모델을 맞춥니다. 이 모델에서 추정된 서브픽셀 정확도의 극한값으로 키포인트의 위치를 업데이트합니다. 다음 업데이트에서 키포인트가 세 방향 중 하나에서 0.5보다 작게 움직일 때까지 최대 5회 반복합니다. 이는 이차 모델이 한 픽셀 위치로 수렴했음을 의미합니다. 두 개의 도우미 함수 `computeGradientAtCenterPixel()`과 `computeHessianAtCenterPixel()`은 모든 3차원에서 그래디언트와 헤시안의 2차 중심 유한차분 근사치를 구현합니다. 여기서 중요한 점은 미적분 시간에 배웠던 일반적인 이차 보간은 균일한 메쉬(픽셀 간격이 균일한 격자)를 사용하기 때문에 잘 작동하지 않는다는 것입니다. 유한차 근사치는 이러한 이산화를 고려하여 보다 정확한 극한값 추정치를 생성합니다.

이제 키포인트를 찾았고 정확하게 위치를 파악했습니다. 여기까지 왔지만 방향 계산과 설명자 생성이라는 두 가지 큰 작업이 남아 있습니다.

-----

# 요약
지금까지 가우스 블러 이미지 피라미드와 가우시안 차 이미지 피라미드를 생성하는 데 사용한 입력 이미지를 흐리게 하는 일련의 스케일을 생성했습니다. 이미지 너비, 높이, 배율 차원을 따라 로컬 최대값과 최소값을 찾아 키포인트 위치를 찾았습니다. 이 키포인트 위치를 이차 보간을 통해 서브픽셀 정확도로 현지화했습니다.

남은 작업은 무엇인가요? 이제 각 키포인트의 방향을 계산해야 합니다. 위치는 같지만 방향이 다른 키포인트도 생성할 수 있습니다. 그런 다음 완성된 키포인트를 정렬하고 중복을 제거합니다. 마지막으로 각 키포인트에 대한 설명자 벡터를 생성하여 키포인트를 식별하고 비교할 수 있도록 합니다.

이 튜토리얼의 마지막에는 템플릿 매칭 데모를 통해 SIFT가 실제로 작동하는 모습을 보여드리겠습니다.

시작해 보겠습니다.

# 키포인트 방향

```
def computeKeypointsWithOrientations(키포인트, 옥타브_인덱스, 가우시안_이미지, 반경_인자=3, 넘버빈=36, 피크_비율=0.8, 스케일_인자=1.5):
    """각 키포인트의 방향을 계산합니다.
    """
    logger.debug('키포인트 방향 계산 중...')
    keypoints_with_orientations = []
    image_shape = 가우스_이미지.모양

    scale = scale_factor * 키포인트_크기 / float32(2 ** (옥타브_인덱스 + 1))  # 현지화ExtremumViaQuadraticFit()의 keypoint.size 계산과 비교합니다.
    radius = int(round(radius_factor * scale))
    weight_factor = -0.5 / (스케일 ** 2)
    raw_histogram = 0(num_bins)
    smooth_histogram = zeros(num_bins)

    범위(-반경, 반경 + 1)의 i에 대해:
        region_y = int(round(keypoint.pt[1] / float32(2 ** 옥타브_인덱스))) + i
        region_y > 0이고 region_y < image_shape[0] - 1인 경우:
            범위(-반경, 반경 + 1)의 j에 대해:
                region_x = int(round(keypoint.pt[0] / float32(2 ** octave_index))) + j
                region_x > 0이고 region_x < image_shape[1] - 1인 경우:
                    dx = 가우시안_이미지[region_y, region_x + 1] - 가우시안_이미지[region_y, region_x - 1]
                    dy = 가우시안_이미지[region_y - 1, region_x] - 가우시안_이미지[region_y + 1, region_x]
                    gradient_magnitude = sqrt(dx * dx + dy * dy)
                    gradient_orientation = rad2deg(arctan2(dy, dx))
                    weight = exp(weight_factor * (i ** 2 + j ** 2))  # 지수 앞의 상수는 나중에 피크를 찾을 것이므로 삭제할 수 있습니다.
                    히스토그램_인덱스 = int(라운드(그라데이션_방향 * num_bins / 360.))
                    raw_histogram[histogram_index % num_bins] += weight * gradient_magnitude

    범위(num_bins)의 n에 대해:
        smooth_histogram[n] = (6 * raw_histogram[n] + 4 * (raw_histogram[n - 1] + raw_histogram[(n + 1) % num_bins]) + raw_histogram[n - 2] + raw_histogram[(n + 2) % num_bins]) / 16.
    orientation_max = max(smooth_histogram)
    orientation_peaks = where(logical_and(smooth_histogram > roll(smooth_histogram, 1), smooth_histogram > roll(smooth_histogram, -1))[0]
    오리엔테이션_피크의 peak_index에 대해:
        peak_value = smooth_histogram[peak_index]
        peak_value >= peak_ratio * orientation_max:
            이차 피크 보간: # 이차 피크 보간
            # 보간 업데이트는 https://ccrma.stanford.edu/~jos/sasp/Quadratic_Interpolation_Spectral_Peaks.html의 방정식 (6.30)에 의해 제공됩니다.
            left_value = smooth_histogram[(peak_index - 1) % num_bins]
            right_value = smooth_histogram[(peak_index + 1) % num_bins]
            보간된_피크_인덱스 = (peak_index + 0.5 * (왼쪽_값 - 오른쪽_값) / (왼쪽_값 - 2 * 피크_값 + 오른쪽_값)) % num_bins
            방향 = 360. - 보간된_피크_인덱스 * 360. / num_bins
            abs(orientation - 360.) < float_tolerance:
                orientation = 0
            new_keypoint = 키포인트(*keypoint.pt, 키포인트.크기, 방향, 키포인트.응답, 키포인트.옥타브)
            keypoints_with_orientations.append(new_keypoint)
    반환 키포인트_위드_오리엔테이션
```

계산 키포인트와 방향()`을 살펴보세요. 여기서 목표는 키포인트의 주변 픽셀에 대한 그라데이션 히스토그램을 만드는 것입니다. 여기서는 OpenCV 구현에서와 같이 정사각형 이웃을 사용한다는 점에 유의하세요. 한 가지 주목해야 할 세부 사항은 기본적으로 3인 '반경 계수(radius_factor)'입니다. 이는 이웃이 각 키포인트의 '3 * 스케일' 이내의 픽셀을 포함한다는 의미이며, 여기서 '스케일'은 가우스 가중치와 관련된 표준 편차입니다. 가우스 분포의 확률 밀도의 99.7%는 3개의 표준 편차 내에 있으며, 이 경우 가중치의 99.7%가 키포인트의 `3 * 스케일` 픽셀 내에 있다는 것을 의미합니다.

다음으로, 이 영역의 각 픽셀에서 2D 그라데이션의 크기와 방향을 계산합니다. 방향에 대해 36개의 빈 히스토그램(빈당 10도)을 생성합니다. 특정 픽셀의 방향은 어떤 히스토그램 구간을 선택할지 알려주지만, 해당 구간에 배치하는 실제 값은 가우시안 가중치를 적용한 해당 픽셀의 그라데이션 크기입니다. 이렇게 하면 키포인트에서 멀리 떨어진 픽셀이 히스토그램에 미치는 영향이 줄어듭니다. 이웃의 모든 픽셀에 대해 이 절차를 반복하여 결과를 동일한 36-빈 히스토그램으로 누적합니다.

모든 작업이 완료되면 히스토그램을 평활화합니다. 평활화 계수는 5점 가우스 필터에 해당하며, 파스칼의 삼각형에서 이러한 계수를 도출하는 방법에 대한 깔끔한 설명을 찾을 수 있습니다.

무슨 일이 일어나고 있는지 시각화하기 위해 템플릿 이미지 'box.png'에 코드를 적용했을 때 생성되는 실제 키포인트에 대한 원시 히스토그램과 평활화된 히스토그램을 아래에 표시했습니다(Github에서 찾을 수 있음).


<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_04.png" width="700px"></p>

<center>원시 방향 히스토그램입니다.</center>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_05.png" width="700px"></p>

<center>빨간색 피크가 있는 가우스 평활 방향 히스토그램.</center>

이제 이 히스토그램에서 SIFT 문서에 지정된 임계값 위에 있는 피크를 찾습니다. 다시 유한 차분인 이차 보간을 사용하여 각 피크의 위치를 찾습니다!  각 피크에 대해 별도의 키포인트를 생성하며, 이러한 키포인트는 방향 속성을 제외하고는 동일합니다. SIFT 논문에서 언급했듯이, 이러한 추가 키포인트는 실제 애플리케이션에서 사용할 때 감지 안정성에 크게 기여합니다.

# 키포인트 정리하기

```
def compareKeypoints(keypoint1, keypoint2):
    """키포인트1이 키포인트2보다 작으면 True를 반환합니다.
    """
    if keypoint1.pt[0] != keypoint2.pt[0]:
        반환 키포인트1.pt[0] - 키포인트2.pt[0]
    if keypoint1.pt[1] != keypoint2.pt[1]:
        반환 keypoint1.pt[1] - keypoint2.pt[1]
    if keypoint1.size != keypoint2.size:
        반환 키포인트2.크기 - 키포인트1.크기
    if keypoint1.angle != keypoint2.angle:
        반환 키포인트1.각도 - 키포인트2.각도
    만약 키포인트1.응답 != 키포인트2.응답:
        반환 키포인트2.응답 - 키포인트1.응답
    if keypoint1.옥타브 != keypoint2.옥타브:
        반환 키포인트2.옥타브 - 키포인트1.옥타브
    반환 키포인트2.class_id - 키포인트1.class_id

def removeDuplicateKeypoints(키포인트):
    """키포인트를 정렬하고 중복된 키포인트를 제거합니다.
    """
    if len(keypoints) < 2:
        반환 키포인트

    keypoints.sort(key=cmp_to_key(compareKeypoints))
    unique_keypoints = [keypoints[0]]

    keypoints[1:]의 next_keypoint에 대해:
        last_unique_keypoint = unique_keypoints[-1]
        if last_unique_keypoint.pt[0] != next_keypoint.pt[0] 또는 \.
           last_unique_keypoint.pt[1] != next_keypoint.pt[1] 또는 \.
           last_unique_keypoint.size != next_keypoint.size 또는 \.
           last_unique_keypoint.angle != next_keypoint.angle:
            unique_keypoints.append(next_keypoint)
    고유 키포인트 반환

def convertKeypointsToInputImageSize(keypoints):
    """키포인트 포인트, 크기, 옥타브를 입력 이미지 크기로 변환합니다.
    """
    converted_keypoints = []
    키포인트의 키포인트에 대해:
        keypoint.pt = 튜플(0.5 * array(keypoint.pt))
        keypoint.size *= 0.5
        keypoint.옥타브 = (keypoint.옥타브 & ~255) | ((keypoint.옥타브 - 1) & 255)
        converted_keypoints.append(키포인트)
    반환 converted_keypoints
```

이제 모든 키포인트를 찾았습니다. 디스크립터 생성으로 넘어가기 전에 몇 가지 정리를 해야 합니다.

removeDuplicateKeypoints()`로 중복을 정렬하고 제거합니다. 키포인트에 사용하는 비교 함수는 OpenCV에서 구현된 것과 동일합니다.

또한 키포인트를 기본 이미지 좌표에서 입력 이미지 좌표로 변환해야 하는데, 이는 관련 속성을 절반으로 줄이면 됩니다.

# 설명자 생성하기

```
def unpackOctave(keypoint):
    """키포인트에서 옥타브, 레이어, 스케일을 계산합니다.
    """
    옥타브 = 키포인트.옥타브 & 255
    layer = (keypoint.octave >> 8) & 255
    옥타브 >= 128이면
        옥타브 = 옥타브 | -128
    scale = 1 / float32(1 << 옥타브) if 옥타브 >= 0 else float32(1 << -옥타브)
    옥타브, 레이어, 스케일 반환

def generateDescriptors(keypoints, gaussian_images, window_width=4, num_bins=8, scale_multiplier=3, descriptor_max_value=0.2):
    """각 키포인트에 대한 설명자 생성
    """
    logger.debug('디스크립터 생성 중...')
    descriptors = []

    키포인트의 키포인트에 대해:
        옥타브, 레이어, 스케일 = unpackOctave(키포인트)
        가우시안_이미지 = 가우시안_이미지[옥타브 + 1, 레이어]
        num_rows, num_cols = 가우시안_이미지.모양
        point = round(scale * array(keypoint.pt)).astype('int')
        bins_per_degree = num_bins / 360.
        각도 = 360. - 키포인트.각도
        cos_angle = cos(deg2rad(angle))
        sin_angle = sin(deg2rad(angle))
        weight_multiplier = -0.5 / ((0.5 * window_width) ** 2)
        row_bin_list = []
        COL_BIN_LIST = []
        magnitude_list = []
        orientation_bin_list = []
        히스토그램_텐서 = 0((window_width + 2, window_width + 2, num_bins))   # 테두리 효과를 고려하기 위해 처음 두 차원이 2씩 증가합니다.

        # 설명자 창 크기(반_폭으로 설명됨)는 OpenCV 규칙을 따릅니다.
        hist_width = scale_multiplier * 0.5 * scale * keypoint.size
        half_width = int(round(hist_width * sqrt(2) * (window_width + 1) * 0.5))   # sqrt(2)는 픽셀의 대각선 길이에 해당합니다.
        half_width = int(min(half_width, sqrt(num_rows ** 2 + num_cols ** 2)))     # 반 너비가 이미지 내에 있는지 확인

        range(-half_width, half_width + 1) 안에 있는지 확인합니다:
            for col in range(-half_width, half_width + 1):
                row_rot = col * sin_angle + row * cos_angle
                col_rot = col * cos_angle - row * sin_angle
                row_bin = (row_rot/hist_width) + 0.5 * window_width - 0.5
                col_bin = (col_rot / hist_width) + 0.5 * window_width - 0.5
                행_빈 > -1이고 행_빈 < window_width 및 col_bin > -1이고 col_bin < window_width인 경우:
                    window_row = int(round(point[1] + row))
                    window_col = int(round(point[0] + col))
                    window_row > 0이고 window_row < num_rows - 1이고 window_col > 0이고 window_col < num_cols - 1인 경우:
                        dx = 가우시안_이미지[window_row, window_col + 1] - 가우시안_이미지[window_row, window_col - 1]
                        dy = 가우시안_이미지[window_row - 1, window_col] - 가우시안_이미지[window_row + 1, window_col]
                        gradient_magnitude = sqrt(dx * dx + dy * dy)
                        그라데이션_방향 = rad2deg(arctan2(dy, dx)) % 360
                        weight = exp(weight_multiplier * ((row_rot / hist_width) ** 2 + (col_rot / hist_width) ** 2))
                        row_bin_list.append(row_bin)
                        col_bin_list.append(col_bin)
                        magnitude_list.append(weight * gradient_magnitude)
                        orientation_bin_list.append((gradient_orientation - angle) * bins_per_degree)

        for row_bin, col_bin, magnitude, orientation_bin in zip(row_bin_list, col_bin_list, magnitude_list, orientation_bin_list):
            # 삼선 보간을 통한 스무딩
            # 표기법은 https://en.wikipedia.org/wiki/Trilinear_interpolation 을 따릅니다.
            # 여기서 실제로는 삼선 보간을 역으로 하고 있다는 점에 유의하세요(큐브의 중심값을 취해 8개의 이웃에 분배합니다).
            row_bin_floor, col_bin_floor, orientation_bin_floor = floor([row_bin, col_bin, orientation_bin]).astype(int)
            row_fraction, col_fraction, orientation_fraction = row_bin - row_bin_floor, col_bin - col_bin_floor, orientation_bin - orientation_bin_floor
            오리엔테이션_빈_바닥이 0이면
                오리엔테이션_빈_바닥 += NUM_BINS
            if orientation_bin_floor >= num_bins:
                오리엔테이션_빈_바닥 -= NUM_BINS

            c1 = magnitude * row_fraction
            c0 = magnitude * (1 - row_fraction)
            C11 = C1 * COL_FRACTION
            C10 = C1 * (1 - COL_FRACTION)
            C01 = C0 * COL_FRACTION
            C00 = C0 * (1 - COL_FRACTION)
            C111 = C11 * 오리엔테이션_프랙션
            C110 = C11 * (1 - 오리엔테이션_프랙션)
            C101 = C10 * 오리엔테이션_프랙션
            C100 = C10 * (1 - 오리엔테이션_프랙션)
            C011 = C01 * 오리엔테이션_프랙션
            C010 = C01 * (1 - 오리엔테이션_프랙션)
            C001 = C00 * 오리엔테이션_프랙션
            C000 = C00 * (1 - 오리엔테이션_프랙션)

            히스토그램_텐서[행_빈_바닥 + 1, col_빈_바닥 + 1, 오리엔테이션_빈_바닥] += c000
            히스토그램_텐서[row_bin_바닥 + 1, col_bin_바닥 + 1, (orientation_bin_바닥 + 1) % num_bins] += c001
            히스토그램_텐서[row_bin_바닥 + 1, col_bin_바닥 + 2, orientation_bin_바닥] += c010
            히스토그램_텐서[row_bin_바닥 + 1, col_bin_바닥 + 2, (orientation_bin_바닥 + 1) % num_bins] += c011
            히스토그램_텐서[row_bin_바닥 + 2, col_bin_바닥 + 1, orientation_bin_바닥] += c100
            히스토그램_텐서[row_bin_바닥 + 2, col_bin_바닥 + 1, (orientation_bin_바닥 + 1) % num_bins] += c101
            히스토그램_텐서[row_bin_바닥 + 2, col_bin_바닥 + 2, orientation_bin_바닥] += c110
            히스토그램_텐서[row_bin_바닥 + 2, col_bin_바닥 + 2, (orientation_bin_바닥 + 1) % num_bins] += c111

        descriptor_vector = histogram_tensor[1:-1, 1:-1, :].flatten() # 히스토그램 테두리를 제거합니다.
        # 설명자_벡터의 임계값을 설정하고 정규화합니다.
        threshold = norm(descriptor_vector) * descriptor_max_value
        descriptor_vector[descriptor_vector > 임계값] = 임계값
        descriptor_vector /= max(norm(descriptor_vector), float_tolerance)
        # 512를 곱하고, 반올림하고, 0에서 255 사이로 포화시켜 float32에서 부호 없는 문자로 변환합니다(OpenCV 규칙).
        descriptor_vector = round(512 * descriptor_vector)
        descriptor_vector[descriptor_vector < 0] = 0
        descriptor_vector[descriptor_vector > 255] = 255
        descriptors.append(descriptor_vector)
    return array(descriptors, dtype='float32')
```

드디어 디스크립터 생성입니다. 디스크립터는 키포인트의 이웃에 대한 정보를 인코딩하고 키포인트 간의 비교를 가능하게 합니다. SIFT 디스크립터는 특히 잘 설계되어 강력한 키포인트 매칭을 가능하게 합니다.

각 키포인트에 대해 첫 번째 단계는 그라데이션 방향의 또 다른 히스토그램을 만드는 것입니다. 각 키포인트 주위에 정사각형 이웃(이번에는 다른 변 길이)을 고려하지만, 이제 이 이웃을 키포인트의 각도에 따라 회전시킵니다. 이것이 바로 SIFT가 회전에 불변하는 이유입니다. 이 회전된 이웃에서 특별한 작업을 수행합니다. 각 픽셀의 위치를 나타내는 이웃에 국한된 인덱스인 행과 열 구간차원을 계산합니다. 또한 키포인트 방향을 계산할 때와 마찬가지로 각 픽셀의 그라데이션 크기와 방향도 계산합니다. 하지만 실제로 히스토그램을 만들고 값을 누적하는 대신 각 픽셀에 대한 히스토그램 구간차원 인덱스와 구간차원 값을 저장하기만 하면 됩니다. 여기서 히스토그램은 이전처럼 36개의 빈이 아니라 360도를 커버할 수 있는 8개의 빈만 있다는 점에 유의하세요.

이제 까다로운 부분이 나옵니다. 2D 배열인 정사각형 이웃을 가지고 각 픽셀을 길이 36의 벡터로 대체한다고 상상해 보겠습니다. 각 픽셀과 관련된 방향 구간은 36 길이 벡터로 인덱싱되고, 이 위치에 이 픽셀에 대한 가중 그라데이션 크기를 저장합니다. 이렇게 하면 `(4, 4, 36)`으로 평가되는 크기 `(window_width, window_width, num_bins)`의 3D 배열이 형성됩니다. 이 3D 배열을 평평하게 만들어 설명자 벡터로 사용하겠습니다. 하지만 그 전에 약간의 평활화를 적용하는 것이 좋습니다.

어떤 종류의 스무딩일까요? 힌트: 더 미세한 차이를 포함합니다. 각 이웃 픽셀의 가중 그라데이션 크기를 행 구간, 열 구간, 방향 구간 등 3차원의 8개의 이웃 픽셀에 분배하여 각 픽셀의 가중 그라데이션 크기를 평활화할 것입니다. 삼선 보간이라는 방법을 사용하거나 더 정확하게는 그 역을 사용할 것입니다. 위키백과에 이 방법에 대한 공식과 시각화가 잘 나와 있습니다. 간단히 말해, 각 이웃 픽셀에는 행 구간차원 인덱스, 열 구간차원 인덱스 및 방향 구간차원 인덱스가 있으며, 히스토그램 값을 8개의 이웃 구간차원에 비례적으로 분배하면서 분배된 부분이 원래 값에 합산되도록 하려는 것입니다.

왜 단순히 히스토그램 값의 8분의 1을 8개의 이웃 빈에 각각 할당하지 않는지 궁금할 수 있습니다. 문제는 이웃 픽셀에 분수 구간차원 인덱스가 있을 수 있다는 사실입니다. 아래 그림에서 빨간색 점으로 표시된 것처럼 각 이웃 픽셀이 3D 점 `[row_bin, col_bin, orientation_bin]`으로 표시된다고 상상해 보십시오. 이 3D 점은 8개의 정수 값 이웃 점인 아래 파란색 점으로 구성된 정육면체의 정확한 중심에 있지 않을 수 있습니다. 빨간색 점의 값을 파란색 점들에 정확하게 분배하려면 빨간색 점에 더 가까운 파란색 점들이 빨간색 점의 값의 더 큰 비율을 받아야 합니다. 이것이 바로 (역의) 삼선 보간이 하는 일입니다. 빨간색 점을 두 개의 녹색 점으로 나누고, 두 개의 녹색 점을 네 개의 점으로 나누고, 마지막으로 네 개의 점을 최종 8개의 점으로 나눕니다.


<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_06.png" width="300px"></p>

<center>삼선 보간의 역을 수행하여 중심에서 벗어날 가능성이 있는 빨간색 점을 취하여 8개의 이웃에 분배합니다(위키백과 이미지).</center>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_07.png" width="300px"></p>

<center>우리는 먼저 빨간색 점을 두 개의 같지 않은 녹색 점으로 나누고 파란색 점에 도달할 때까지 재귀합니다(Wikipedia의 이미지).</center>

마지막 단계는 평활화된 3D 배열을 길이 128의 설명자 벡터로 평평하게 만드는 것입니다. 그런 다음 임계값을 적용하고 정규화합니다. 그런 다음 OpenCV 구현에서는 나중에 설명자를 비교할 때 효율성을 위해 설명자의 크기를 조정하고 0에서 255 사이에 위치하도록 포화시킵니다.

이 절차를 반복하여 각 키포인트에 대해 하나의 디스크립터 벡터를 생성합니다.

이것이 전체 SIFT 알고리즘입니다. 많이 어렵겠지만 여기까지 오셨습니다! 이제 템플릿 매칭 데모를 통해 SIFT가 실제로 어떻게 작동하는지 살펴보겠습니다.

# 애플리케이션: 템플릿 매칭
```
import numpy as np
import cv2
import pysift
matplotlib에서 pyplot을 plt로 가져옵니다.
logging 임포트
logger = logging.getLogger(__name__)

min_match_count = 10

img1 = cv2.imread('box.png', 0) # queryImage
img2 = cv2.imread('box_in_scene.png', 0) # trainImage

# SIFT 키포인트와 설명자 계산하기
kp1, des1 = pysift.computeKeypointsAndDescriptors(img1)
kp2, des2 = pysift.computeKeypointsAndDescriptors(img2)

# FLANN 초기화 및 사용
flann_index_kdtree = 0
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks = 50)
flann = cv2.FlannBasedMatcher(index_params, search_params)
matches = flann.knnMatch(des1, des2, k=2)

# 로우 비율 테스트
good = []
일치 항목의 m, n에 대해
    if m.distance < 0.7 * n.distance:
        good.append(m)

len(good) > MIN_MATCH_COUNT:
    # 템플릿과 장면 사이의 동형성 추정
    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

    M = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)[0]

    # 장면 이미지에 감지된 템플릿 그리기
    h, w = img1.shape
    pts = np.float32([[0, 0],
                      [0, h - 1],
                      [w - 1, h - 1],
                      [w - 1, 0]]).reshape(-1, 1, 2)
    dst = cv2.perspectiveTransform(pts, M)

    img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)

    h1, w1 = img1.shape
    h2, w2 = img2.shape
    n폭 = w1 + w2
    n높이 = 최대(h1, h2)
    hdif = int((h2 - h1) / 2)
    newimg = np.zeros((nHeight, nWidth, 3), np.uint8)

    범위(3)의 i에 대해
        newimg[hdif:hdif + h1, :w1, i] = img1
        newimg[:h2, w1:w1 + w2, i] = img2

    # SIFT 키포인트 일치 항목 그리기
    를 그립니다:
        pt1 = (int(kp1[m.queryIdx].pt[0]), int(kp1[m.queryIdx].pt[1] + hdif))
        pt2 = (int(kp2[m.trainIdx].pt[0] + w1), int(kp2[m.trainIdx].pt[1]))
        cv2.line(newimg, pt1, pt2, (255, 0, 0))

    plt.imshow(newimg)
    plt.show()
else:
    print("일치하는 항목이 충분하지 않습니다 - %d/%d" % (len(good), MIN_MATCH_COUNT))
```
리포지토리의 이 스크립트는 OpenCV의 템플릿 매칭 데모를 각색한 것입니다. 템플릿 이미지가 주어지면 장면 이미지에서 이를 감지하고 동형성을 계산하는 것이 목표입니다. 템플릿 이미지와 장면 이미지 모두에서 SIFT 키포인트와 설명자를 계산하고, 두 설명자 세트에서 근사 근접 이웃 검색을 수행하여 유사한 키포인트를 찾습니다. 임계값보다 가까운 키포인트 쌍은 잘 일치하는 것으로 간주됩니다. 마지막으로, 일치하는 키포인트에 대해 RANSAC을 수행하여 가장 적합한 호모그래피를 계산합니다.

한 번 해보세요. 다음과 같은 결과가 나올 것입니다:

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_08.png" width="600px"></p>

<center>키포인트 일치 항목과 함께 감지된 템플릿을 보여주는 template_matching_demo.py의 출력 플롯</center>

자체 템플릿과 장면 이미지로 이 스크립트를 사용해 SIFT 키포인트의 안정성을 느껴보세요. 어떤 조건에서 잘 작동하고 어떤 조건에서 성능이 저하되는지 살펴볼 수 있습니다.

# 마무리

다 끝났습니다! SIFT는 모든 컴퓨터 비전 엔지니어의 툴킷에 없어서는 안 될 구성 요소입니다. 이 튜토리얼을 통해 SIFT를 종이 위의 멋진 아이디어에서 실용적이고 강력한 코드로 끌어올리는 무수한 구현 세부 사항을 살펴볼 수 있었기를 바랍니다. 단독으로 사용하든, 더 큰 이미지 처리 파이프라인의 일부로 사용하든, 학습 기반 접근 방식에 대한 벤치마크로 사용하든 향후 비전 프로젝트에 SIFT를 사용해 보는 것을 잊지 마세요.
