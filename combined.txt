# The Code

# 코드

You can find my Python implementation of SIFT here. In this tutorial, we’ll walk through this code (the file pysift.py) step by step, printing and visualizing variables along the way to help us fully understand what’s happening at every moment. I wrote this implementation by closely following the OpenCV implementation, simplifying and Pythonizing the logic without sacrificing any details.

제가 구현한 SIFT의 Python 코드는 여기에서 찾을 수 있습니다. 이 튜토리얼에서는 이 코드(파일 pysift.py)를 단계별로 살펴보면서 매 순간 무슨 일이 일어나는지 완전히 이해할 수 있도록 변수를 인쇄하고 시각화해 보겠습니다. 저는 이 구현을 OpenCV 구현을 면밀히 따르면서 로직을 단순화하고 파이썬화하여 세부 사항을 희생하지 않고 작성했습니다.





The usage is simple:

사용법은 간단합니다:





```

```

import cv2

import cv2

import pysift

import pysift





image = cv2.imread('your_image.png', 0)

image = cv2.imread('your_image.png', 0)

keypoints, descriptors = pysift.computeKeypointsAndDescriptors(image)

키포인트, 설명자 = pysift.computeKeypointsAndDescriptors(이미지)

```

```





Simply pass a 2D NumPy array to `computeKeypointsAndDescriptors()` to return a list of OpenCV `KeyPoint` objects and a list of the associated 128-length descriptor vectors. This way `pysift` works as a drop-in replacement for OpenCV’s SIFT functionality. Note that this implementation is meant to be clear and easy to understand, and it’s not designed for high performance. It’ll take a couple of minutes to process an ordinary input image on an ordinary laptop.

2D NumPy 배열을 `computKeypointsAndDescriptors()`에 전달하기만 하면 OpenCV `KeyPoint` 객체 목록과 연결된 128 길이의 설명자 벡터 목록을 반환합니다. 이렇게 하면 `pysift`가 OpenCV의 SIFT 기능을 대체하는 드롭인 방식으로 작동합니다. 이 구현은 명확하고 이해하기 쉽게 하기 위한 것이며, 고성능을 위해 설계된 것은 아닙니다. 일반 노트북에서 일반적인 입력 이미지를 처리하는 데 몇 분 정도 걸립니다.





Clone the repo and try out the template matching demo. You’ll get almost the same keypoints you’d get using OpenCV (the differences are due to floating point error).

리포지토리를 복제하고 템플릿 매칭 데모를 사용해 보세요. OpenCV를 사용할 때와 거의 동일한 키포인트를 얻을 수 있습니다(차이는 부동 소수점 오류로 인한 것입니다).





# SIFT Theory and Overview

# SIFT 이론 및 개요

Let’s briefly go over the reasoning behind SIFT and develop a high-level roadmap of the algorithm. I won’t dwell on the math. You can (and should) read the original paper [here](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf).

SIFT의 이론적 배경을 간단히 살펴보고 알고리즘의 개략적인 로드맵을 만들어 보겠습니다. 수학적인 내용은 다루지 않겠습니다. 원본 논문은 [여기](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)에서 읽어보실 수 있으며, 반드시 읽어보셔야 합니다.





SIFT identifies keypoints that are distinctive across an image’s width, height, and most importantly, scale. By considering scale, we can identify keypoints that will remain stable (to an extent) even when the template of interest changes size, when the image quality becomes better or worse, or when the template undergoes changes in viewpoint or aspect ratio. Moreover, each keypoint has an associated orientation that makes SIFT features invariant to template rotations. Finally, SIFT will generate a descriptor for each keypoint, a 128-length vector that allows keypoints to be compared. These descriptors are nothing more than a histogram of gradients computed within the keypoint’s neighborhood.

SIFT는 이미지의 너비, 높이, 그리고 가장 중요한 축척에 걸쳐 특징적인 키포인트를 식별합니다. 스케일을 고려하면 관심 있는 템플릿의 크기가 변경되거나 이미지 품질이 좋아지거나 나빠지거나 템플릿의 시점이나 종횡비가 변경되더라도 어느 정도 안정적으로 유지되는 키포인트를 식별할 수 있습니다. 또한 각 키포인트에는 템플릿 회전과 무관하게 SIFT 기능이 작동하도록 하는 관련 방향이 있습니다. 마지막으로, SIFT는 각 키포인트에 대한 설명자(키포인트를 비교할 수 있는 128 길이의 벡터)를 생성합니다. 이러한 설명자는 키포인트의 주변에서 계산된 그라데이션의 히스토그램에 불과합니다.





Most of the tricky details in SIFT relate to scale space, like applying the correct amount of blur to the input image, or converting keypoints from one scale to another.

SIFT의 까다로운 세부 사항은 대부분 입력 이미지에 정확한 양의 블러를 적용하거나 키포인트를 한 축척에서 다른 축척으로 변환하는 등 축척 공간과 관련이 있습니다.





Below you can see `pysift`’s main function, `computeKeypointsAndDescriptors()`, which gives you a clear overview of the different components involved in SIFT. First, we call `generateBaseImage()` to appropriately blur and double the input image to produce the base image of our “image pyramid”, a set of successively blurred and downsampled images that form our scale space. We then call `computeNumberOfOctaves()` to compute the number of layers (“octaves”) in our image pyramid. Now we can actually build the image pyramid. We start with `generateGaussianKernels()` to create a list of scales (gaussian kernel sizes) that is passed to `generateGaussianImages()`, which repeatedly blurs and downsamples the base image. Next we subtract adjacent pairs of gaussian images to form a pyramid of difference-of-Gaussian (“DoG”) images. We’ll use this final DoG image pyramid to identify keypoints using `findScaleSpaceExtrema()`. We’ll clean up these keypoints by removing duplicates and converting them to the input image size. Finally, we’ll generate descriptors for each keypoint via `generateDescriptors()`.

아래에서 `pysift`의 주요 함수인 `computeKeypointsAndDescriptors()`를 보면 SIFT에 관련된 다양한 구성 요소를 명확하게 파악할 수 있습니다. 먼저 `generateBaseImage()`를 호출하여 입력 이미지를 적절히 흐리게 하고 두 배로 늘려 스케일 공간을 형성하는 연속적으로 흐리게 하고 다운샘플링한 이미지 집합인 "이미지 피라미드"의 기본 이미지를 생성합니다. 그런 다음 `computeNumberOfOctaves()`를 호출하여 이미지 피라미드의 레이어 수("옥타브")를 계산합니다. 이제 이미지 피라미드를 실제로 만들 수 있습니다. 가우스 커널 크기(가우스 커널 크기)의 목록을 생성하기 위해 `generateGaussianKernels()`로 시작하여 기본 이미지를 반복적으로 흐리게 하고 다운샘플링하는 `generateGaussianImages()`로 전달합니다. 다음으로 인접한 가우시안 이미지 쌍을 빼서 가우시안 차이("DoG") 이미지의 피라미드를 형성합니다. 이 최종 DoG 이미지 피라미드를 사용하여 `findScaleSpaceExtrema()`를 사용하여 키포인트를 식별합니다. 중복을 제거하고 입력 이미지 크기로 변환하여 이러한 키포인트를 정리하겠습니다. 마지막으로 `generateDescriptors()`를 통해 각 키포인트에 대한 설명자를 생성합니다.





```

```

from numpy import all, any, array, arctan2, cos, sin, exp, dot, log, logical_and, roll, sqrt, stack, trace, unravel_index, pi, deg2rad, rad2deg, where, zeros, floor, full, nan, isnan, round, float32

import all, any, array, arctan2, cos, sin, exp, dot, log, logical_and, roll, sqrt, stack, trace, unravel_index, pi, deg2rad, rad2deg, where, zeros, floor, full, nan, isnan, round, float32 from numpy.

from numpy.linalg import det, lstsq, norm

numpy.linalg에서 det, lstsq, norm을 가져옵니다.

from cv2 import resize, GaussianBlur, subtract, KeyPoint, INTER_LINEAR, INTER_NEAREST

cv2에서 크기 조정, 가우시안 블러, 빼기, 키포인트, inter_linear, inter_nearest를 가져옵니다.

from functools import cmp_to_key

함수 도구에서 cmp_to_key를 가져옵니다.

import logging

import logging





####################

####################

# Global variables #

전역 변수 ## 전역 변수

####################

####################





logger = logging.getLogger(__name__)

logger = logging.getLogger(__name__)

float_tolerance = 1e-7

float_tolerance = 1e-7





#################

#################

# Main function #

주요 함수 ## 메인 함수

#################

#################





def computeKeypointsAndDescriptors(image, sigma=1.6, num_intervals=3, assumed_blur=0.5, image_border_width=5):

def computeKeypointsAndDescriptors(이미지, 시그마=1.6, 넘버_간격=3, 가정된_흐림=0.5, 이미지_경계_폭=5):

    """Compute SIFT keypoints and descriptors for an input image

    """입력 이미지에 대한 SIFT 키포인트와 디스크립터를 계산합니다.

    """

    """

    image = image.astype('float32')

    image = image.astype('float32')

    base_image = generateBaseImage(image, sigma, assumed_blur)

    base_image = generateBaseImage(image, sigma, assumed_blur)

    num_octaves = computeNumberOfOctaves(base_image.shape)

    num_octaves = computeNumberOfOctaves(base_image.shape)

    gaussian_kernels = generateGaussianKernels(sigma, num_intervals)

    가우시안 커널 = generateGaussianKernels(sigma, num_intervals)

    gaussian_images = generateGaussianImages(base_image, num_octaves, gaussian_kernels)

    가우시안_이미지 = generateGaussianImages(base_image, num_octaves, gaussian_kernels)

    dog_images = generateDoGImages(gaussian_images)

    dog_images = generateDoGImages(가우시안_이미지)

    keypoints = findScaleSpaceExtrema(gaussian_images, dog_images, num_intervals, sigma, image_border_width)

    키포인트 = findScaleSpaceExtrema(가우시안_이미지, dog_이미지, num_intervals, 시그마, image_border_width)

    keypoints = removeDuplicateKeypoints(keypoints)

    키포인트 = 제거중복키포인트(키포인트)

    keypoints = convertKeypointsToInputImageSize(keypoints)

    키포인트 = convertKeypointsToInputImageSize(키포인트)

    descriptors = generateDescriptors(keypoints, gaussian_images)

    설명자 = 생성 설명자(키포인트, 가우시안_이미지)

    return keypoints, descriptors

    반환 키포인트, 설명자

```

```





Simple enough. Now we’ll explore these functions one at a time.

충분히 간단합니다. 이제 이러한 함수를 한 번에 하나씩 살펴보겠습니다.





# Scale Space and Image Pyramids

# 스케일 스페이스 및 이미지 피라미드

```

```

def generateBaseImage(image, sigma, assumed_blur):

def generateBaseImage(image, sigma, assumed_blur):

    """Generate base image from input image by upsampling by 2 in both directions and blurring

    """입력 이미지에서 양방향으로 2씩 업샘플링하고 흐리게 처리하여 베이스 이미지를 생성합니다.

    """

    """

    logger.debug('Generating base image...')

    logger.debug('기본 이미지 생성 중...')

    image = resize(image, (0, 0), fx=2, fy=2, interpolation=INTER_LINEAR)

    image = resize(image, (0, 0), fx=2, fy=2, interpolation=INTER_LINEAR)

    sigma_diff = sqrt(max((sigma ** 2) - ((2 * assumed_blur) ** 2), 0.01))

    sigma_diff = sqrt(max((sigma ** 2) - ((2 * assumed_blur) ** 2), 0.01))

    return GaussianBlur(image, (0, 0), sigmaX=sigma_diff, sigmaY=sigma_diff)  # the image blur is now sigma instead of assumed_blur

    return GaussianBlur(image, (0, 0), sigmaX=sigma_diff, sigmaY=sigma_diff)  # 이제 이미지 블러는 가정된_블러 대신 시그마가 됩니다.





def computeNumberOfOctaves(image_shape):

def computeNumberOfOctaves(image_shape):

    """Compute number of octaves in image pyramid as function of base image shape (OpenCV default)

    """기본 이미지 모양에 따라 이미지 피라미드에서 옥타브 수를 계산합니다(OpenCV 기본값).

    """

    """

    return int(round(log(min(image_shape)) / log(2) - 1))

    반환 int(round(log(min(image_shape))) / log(2) - 1))





def generateGaussianKernels(sigma, num_intervals):

def generateGaussianKernels(sigma, num_intervals):

    """Generate list of gaussian kernels at which to blur the input image. Default values of sigma, intervals, and octaves follow section 3 of Lowe's paper.

    """입력 이미지를 흐리게 할 가우시안 커널 목록을 생성합니다. 시그마, 간격, 옥타브의 기본값은 Lowe의 논문 섹션 3을 따릅니다.

    """

    """

    logger.debug('Generating scales...')

    logger.debug('스케일 생성 중...')

    num_images_per_octave = num_intervals + 3

    num_images_per_옥타브 = num_intervals + 3

    k = 2 ** (1. / num_intervals)

    k = 2 ** (1. / num_intervals)

    gaussian_kernels = zeros(num_images_per_octave)  # scale of gaussian blur necessary to go from one blur scale to the next within an octave

    가우시안 커널 = 0(num_images_per_octave) # 한 옥타브 내에서 한 블러 스케일에서 다음 블러 스케일로 이동하는 데 필요한 가우시안 블러 스케일

    gaussian_kernels[0] = sigma

    가우시안 커널[0] = 시그마





    for image_index in range(1, num_images_per_octave):

    범위(1, num_images_per_옥타브)의 image_index에 대해:

        sigma_previous = (k ** (image_index - 1)) * sigma

        sigma_previous = (k ** (image_index - 1)) * sigma

        sigma_total = k * sigma_previous

        시그마_총계 = k * 시그마_이전

        gaussian_kernels[image_index] = sqrt(sigma_total ** 2 - sigma_previous ** 2)

        가우스 커널[이미지_인덱스] = sqrt(sigma_total ** 2 - sigma_previous ** 2)

    return gaussian_kernels

    반환 가우시안 커널





def generateGaussianImages(image, num_octaves, gaussian_kernels):

def generateGaussianImages(image, num_octaves, gaussian_kernels):

    """Generate scale-space pyramid of Gaussian images

    """가우시안 이미지의 스케일-공간 피라미드를 생성합니다.

    """

    """

    logger.debug('Generating Gaussian images...')

    logger.debug('가우시안 이미지 생성 중...')

    gaussian_images = []

    가우시안 이미지 = []





    for octave_index in range(num_octaves):

    범위(num_octaves)의 옥타브_인덱스에 대해:

        gaussian_images_in_octave = []

        가우시안_이미지_in_옥타브 = []

        gaussian_images_in_octave.append(image)  # first image in octave already has the correct blur

        gaussian_images_in_octave.append(image) # 옥타브의 첫 번째 이미지에 이미 올바른 블러가 있습니다.

        for gaussian_kernel in gaussian_kernels[1:]:

        가우시안_커널의 가우시안_커널[1:]:

            image = GaussianBlur(image, (0, 0), sigmaX=gaussian_kernel, sigmaY=gaussian_kernel)

            image = GaussianBlur(image, (0, 0), sigmaX=gaussian_kernel, sigmaY=gaussian_kernel)

            gaussian_images_in_octave.append(image)

            가우시안_이미지_in_옥타브.append(이미지)

        gaussian_images.append(gaussian_images_in_octave)

        가우시안_이미지.append(가우시안_이미지_in_옥타브)

        octave_base = gaussian_images_in_octave[-3]

        옥타브_베이스 = 가우시안_이미지_인_옥타브[-3]

        image = resize(octave_base, (int(octave_base.shape[1] / 2), int(octave_base.shape[0] / 2)), interpolation=INTER_NEAREST)

        image = resize(octave_base, (int(octave_base.shape[1] / 2), int(octave_base.shape[0] / 2)), interpolation=INTER_NEAREST)

    return array(gaussian_images)

    반환 배열(가우시안_이미지)





def generateDoGImages(gaussian_images):

def generateDoGImages(가우시안_이미지):

    """Generate Difference-of-Gaussians image pyramid

    """가우시안 차분 이미지 피라미드 생성

    """

    """

    logger.debug('Generating Difference-of-Gaussian images...')

    logger.debug('가우시안 차 이미지 생성 중...')

    dog_images = []

    dog_images = []





    for gaussian_images_in_octave in gaussian_images:

    가우시안 이미지의 가우시안_이미지_in_옥타브에 대해:

        dog_images_in_octave = []

        도그_이미지_in_옥타브 = []

        for first_image, second_image in zip(gaussian_images_in_octave, gaussian_images_in_octave[1:]):

        FOR FIRST_IMAGE, SECOND_IMAGE IN ZIP(GAUSIAN_IMAGES_IN_옥타브, GAUSIAN_IMAGES_IN_옥타브[1:]):

            dog_images_in_octave.append(subtract(second_image, first_image))  # ordinary subtraction will not work because the images are unsigned integers

            dog_images_in_octave.append(subtract(second_image, first_image))  # 이미지가 부호가 없는 정수이므로 일반 뺄셈은 작동하지 않습니다.

        dog_images.append(dog_images_in_octave)

        dog_images.append(dog_images_in_octave)

    return array(dog_images)

    반환 array(dog_images)

```

```





Our first step is `generateBaseImage()`, which simply doubles the input image in size and applies Gaussian blur. Assuming the input image has a blur of `assumed_blur = 0.5`, if we want our resulting base image to have a blur of `sigma`, we need to blur the doubled input image by `sigma_diff`. Note that blurring an input image by kernel size $σ_1$ and then blurring the resulting image by $σ_2$ is equivalent to blurring the input image just once by $σ$, where $σ^2 = σ_1^2 + σ_2^2$. (Interested readers can find a proof here.)

첫 번째 단계는 입력 이미지의 크기를 두 배로 늘리고 가우시안 블러를 적용하는 `generateBaseImage()`입니다. 입력 이미지의 블러가 `assumed_blur = 0.5`라고 가정할 때, 결과 베이스 이미지의 블러를 `sigma`로 만들려면 두 배가 된 입력 이미지에 `sigma_diff`만큼 블러를 적용해야 합니다. 입력 이미지를 커널 크기 $σ_1$만큼 흐리게 한 다음 결과 이미지를 $σ_2$만큼 흐리게 하는 것은 $σ^2 = σ_1^2 + σ_2^2$에서 입력 이미지를 $σ$만큼 한 번만 흐리게 하는 것과 동일하다는 점에 유의하세요. (관심 있는 독자는 여기에서 증명을 찾을 수 있습니다.)





Now let’s look at `computeNumberOfOctaves()`, which is simple but requires some explanation. This function computes the number of times we can repeatedly halve an image until it becomes too small. Well, for starters, the final image should have a side length of at least 1 pixel. We can set up an equation for this. If $y$ is the shorter side length of the image, then we have $y / 2^x = 1$, where $x$ is the number of times we can halve the base image. We can take the logarithm of both sides and solve for $x$ to obtain $\log(y) / \log(2)$. So why does the $-1$ show up in the function above? At the end of the day, we have to round $x$ down to the nearest integer (floor$(x)$) to have an integer number of layers in our image pyramid.

이제 간단하지만 약간의 설명이 필요한 `computeNumberOfOctaves()`를 살펴보겠습니다. 이 함수는 이미지가 너무 작아질 때까지 반복적으로 이미지를 반으로 줄일 수 있는 횟수를 계산합니다. 우선, 최종 이미지의 측면 길이는 1픽셀 이상이어야 합니다. 이에 대한 방정식을 설정할 수 있습니다. y$가 이미지의 짧은 측면 길이라면 $y / 2^x = 1$이 되고, 여기서 $x$는 기본 이미지를 절반으로 줄일 수 있는 횟수입니다. 양변의 로그를 취하고 $x$를 풀면 $\log(y) / \log(2)$를 구할 수 있습니다. 그렇다면 위의 함수에서 왜 $-1$이 나타날까요? 결국 이미지 피라미드에 정수의 레이어 수를 가지려면 $x$를 가장 가까운 정수(floor$(x)$로 반내림해야 합니다.





Actually, if you look at how `numOctaves` is used in the functions below, we halve the base image `numOctaves — 1` times to end up with `numOctaves` layers, including the base image. This ensures the image in the highest octave (the smallest image) will have a side length of at least 3. This is important because we’ll search for minima and maxima in each DoG image later, which means we need to consider 3-by-3 pixel neighborhoods.

실제로 아래 함수에서 `numOctaves`가 어떻게 사용되는지 살펴보면, 기본 이미지 `numOctaves - 1`을 반으로 줄여 기본 이미지를 포함한 `numOctaves` 레이어로 끝납니다. 이렇게 하면 가장 높은 옥타브(가장 작은 이미지)의 이미지의 측면 길이가 최소 3이 됩니다. 이는 나중에 각 DoG 이미지에서 최소값과 최대값을 검색할 때 3×3픽셀 이웃을 고려해야 하기 때문에 중요합니다.





Next we `generateGaussianKernels()`, which creates a list of the amount of blur for each image in a particular layer. Note that the image pyramid has `numOctaves` layers, but each layer itself `has numIntervals + 3` images. All the images in the same layer have the same width and height, but the amount of blur successively increases. Where does the `+ 3` come from? We have `numIntervals + 1` images to cover `numIntervals` steps from one blur value to twice that value. We have another `+ 2` for one blur step before the first image in the layer and another blur step after the last image in the layer. We need these two extra images at the end because we’ll subtract adjacent Gaussian images to create a DoG image pyramid. This means that if we compare images from two neighboring layers, we’ll see many of the same blur values repeated. We need this repetition to make sure we cover all blur steps when we subtract images.

다음으로 특정 레이어에 있는 각 이미지의 블러 양 목록을 생성하는 `generateGaussianKernels()`를 호출합니다. 이미지 피라미드에는 `numOctaves` 레이어가 있지만 각 레이어 자체에는 `numIntervals + 3` 이미지가 있다는 점에 유의하세요. 같은 레이어에 있는 모든 이미지의 너비와 높이는 동일하지만 흐림의 양은 연속적으로 증가합니다. '+ 3'은 어디에서 오는 것일까요? 하나의 흐림 값에서 그 값의 두 배까지 `numIntervals` 단계를 커버하기 위해 `numIntervals + 1` 이미지가 있습니다. 레이어의 첫 번째 이미지 앞에 하나의 블러 단계와 레이어의 마지막 이미지 뒤에 또 다른 블러 단계를 위한 또 다른 `+ 2`가 있습니다. 마지막에 이 두 개의 추가 이미지가 필요한 이유는 인접한 가우시안 이미지를 빼서 DoG 이미지 피라미드를 생성하기 때문입니다. 즉, 인접한 두 레이어의 이미지를 비교하면 동일한 블러 값이 많이 반복되는 것을 볼 수 있습니다. 이미지를 뺄 때 모든 블러 단계를 커버하려면 이러한 반복이 필요합니다.





Let’s stop for a minute and print out the generated kernels:

잠시 멈추고 생성된 커널을 인쇄해 보겠습니다:





```

```

print(gaussian_kernels)

print(가우시안_커널)

array([1.6, 1.22627, 1.54501, 1.94659, 2.45255, 3.09002])

array([1.6, 1.22627, 1.54501, 1.94659, 2.45255, 3.09002])

```

```





That’s weird — how come we drop from `1.6` to `1.22627` before increasing again? Take another good look at the `generateGaussianKernels()` function. The first element of this array is simply our starting `sigma`, but after that each element is the additional scale we need to convolve with the previous scale. To be concrete, we start out with an image with scale `1.6`. We blur this image with a Gaussian kernel of `1.22627`, which produces an image with a blur of `sqrt(1.6 ** 2 + 1.22627 ** 2) == 2.01587` , and we blur this new image with a kernel of size `1.54501` to produce a third image of blur `sqrt(2.01587 ** 2 + 1.54501 ** 2) == 2.53984`. Finally, we blur this image by `1.94659` to produce our last image, which has a blur of `sqrt(2.53984 ** 2 + 1.94659 ** 2) == 3.2`. But `2 * 1.6 == 3.2`, so we’ve moved up exactly one octave!

이상하네요 - 어떻게 `1.6`에서 `1.22627`로 떨어졌다가 다시 증가하나요? 생성 가우시안 커널()` 함수를 다시 한 번 잘 살펴봅시다. 이 배열의 첫 번째 요소는 단순히 시작 `시그마`이지만, 그 이후 각 요소는 이전 스케일로 컨볼브하는 데 필요한 추가 스케일입니다. 구체적으로 설명하자면 배율이 `1.6`인 이미지로 시작합니다. 이 이미지를 `1.22627`의 가우스 커널로 블러링하여 `sqrt(1.6 ** 2 + 1.22627 ** 2) == 2.01587`의 블러 이미지를 생성하고, 이 새 이미지를 `1.54501` 크기의 커널로 블러링하여 `sqrt(2.01587 ** 2 + 1.54501 ** 2) == 2.53984`의 세 번째 블러 이미지를 생성합니다. 마지막으로 이 이미지를 `1.94659`만큼 흐리게 하여 마지막 이미지를 생성하면 `sqrt(2.53984 ** 2 + 1.94659 ** 2) == 3.2`의 흐리게 처리된 이미지가 생성됩니다. 하지만 `2 * 1.6 == 3.2`이므로 정확히 한 옥타브 올라간 것입니다!





Now we have all we need to actually generate our image pyramids. We `generateGaussianImages()` by starting with our base image and successively blurring it according to our `gaussian_kernels`. Note that we skip the first element of `gaussian_kernels` because we begin with an image that already has that blur value. We halve the third-to-last image, since this has the appropriate blur we want, and use this to begin the next layer. This way we get that nice overlap mentioned previously. Finally, we `generateDoGImages()` by subtracting successive pairs of these Gaussian-blurred images. Careful — although ordinary subtraction will work here because we’ve cast the input image to `float32`, we’ll use OpenCV’s `subtract()` function so that the code won’t break if you choose to remove this cast and pass in `uint` type images.

이제 이미지 피라미드를 실제로 생성하는 데 필요한 모든 것이 준비되었습니다. 기본 이미지로 시작하여 '가우시안_커널'에 따라 연속적으로 흐리게 처리하는 방식으로 `GaussianImages()`를 생성합니다. 이미 해당 블러 값이 있는 이미지로 시작하기 때문에 `gaussian_kernels`의 첫 번째 요소는 건너뜁니다. 세 번째에서 마지막 이미지에는 원하는 적절한 블러가 있으므로 이를 반으로 줄이고 다음 레이어를 시작할 때 사용합니다. 이렇게 하면 앞서 언급한 멋진 겹침 효과를 얻을 수 있습니다. 마지막으로 가우시안 블러 처리된 이미지의 연속적인 쌍을 빼서 `generateDoGImages()`를 호출합니다. 주의 - 여기서는 입력 이미지를 `float32`로 캐스팅했기 때문에 일반적인 뺄셈이 작동하지만, 이 캐스팅을 제거하고 `uint` 유형 이미지를 전달해도 코드가 손상되지 않도록 OpenCV의 `subtract()` 함수를 사용할 것입니다.





Below I’ve plotted the third layer of our Gaussian pyramid, `gaussian_images[2]`. Note how the images get progressively smoother, while finer features disappear. I’ve also plotted `dog_images[2]`. Remember that `dog_images[2][i] = gaussian_images[2][i + 1] — gaussian_images[2][i]`, and notice how the DoG images look like edge maps.

아래에는 가우스 피라미드의 세 번째 레이어인 `gaussian_images[2]`를 플롯했습니다. 이미지가 점점 더 부드러워지고 미세한 특징이 사라지는 것을 볼 수 있습니다. 또한 `dog_images[2]`를 플롯했습니다. dog_images[2][i] = 가우시안 이미지[2][i + 1] - 가우시안 이미지[2][i]`를 기억하고, DoG 이미지가 어떻게 에지 맵처럼 보이는지 확인합니다.





<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_01.png" width="840px"></p>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_01.png" width="840px"></p>





<center>Images from the third layer of our Gaussian image pyramid.</center>

<center>가우시안 이미지 피라미드의 세 번째 레이어에서 가져온 이미지입니다.</center>





<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_02.png" width="840px"></p>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_02.png" width="840px"></p>





<center>Images from the third layer of our difference-of-Gaussians image pyramid.</center>

<center>가우시안 차이 이미지 피라미드의 세 번째 레이어에서 가져온 이미지입니다.</center>





Run the code and plot these images yourself to verify that you obtain similar results.

코드를 실행하고 이 이미지를 직접 플롯하여 비슷한 결과가 나오는지 확인해 보세요.





At last we’ve got our DoG image pyramid. It’s time to find our keypoints.

드디어 DoG 이미지 피라미드가 완성되었습니다. 이제 키포인트를 찾을 차례입니다.





# Finding Scale Space Extrema

# 스케일 공간 극한점 찾기

```

```

def findScaleSpaceExtrema(gaussian_images, dog_images, num_intervals, sigma, image_border_width, contrast_threshold=0.04):

def findScaleSpaceExtrema(가우시안_이미지, 도그_이미지, num_intervals, 시그마, 이미지_경계_폭, 대비_임계값=0.04):

    """Find pixel positions of all scale-space extrema in the image pyramid

    """이미지 피라미드에서 모든 스케일-공간 극값의 픽셀 위치를 찾습니다.

    """

    """

    logger.debug('Finding scale-space extrema...')

    logger.debug('스케일-공간 극한값 찾기...')

    threshold = floor(0.5 * contrast_threshold / num_intervals * 255)  # from OpenCV implementation

    threshold = floor(0.5 * contrast_threshold / num_intervals * 255) # OpenCV 구현에서 가져옴

    keypoints = []

    키포인트 = []





    for octave_index, dog_images_in_octave in enumerate(dog_images):

    for 옥타브_인덱스, 도그이미지_인_옥타브 in enumerate(dog_images):

        for image_index, (first_image, second_image, third_image) in enumerate(zip(dog_images_in_octave, dog_images_in_octave[1:], dog_images_in_octave[2:])):

        for image_index, (first_image, second_image, third_image) in enumerate(zip(dog_images_in_octave, dog_images_in_octave[1:], dog_images_in_octave[2:])):

            # (i, j) is the center of the 3x3 array

            # (i, j)는 3x3 배열의 중심입니다.

            for i in range(image_border_width, first_image.shape[0] - image_border_width):

            for i in range(image_border_width, first_image.shape[0] - image_border_width):

                for j in range(image_border_width, first_image.shape[1] - image_border_width):

                for j in range(image_border_width, first_image.shape[1] - image_border_width)::

                    if isPixelAnExtremum(first_image[i-1:i+2, j-1:j+2], second_image[i-1:i+2, j-1:j+2], third_image[i-1:i+2, j-1:j+2], threshold):

                    if isPixelAnExtremum(first_image[i-1:i+2, j-1:j+2], second_image[i-1:i+2, j-1:j+2], third_image[i-1:i+2, j-1:j+2], threshold):

                        localization_result = localizeExtremumViaQuadraticFit(i, j, image_index + 1, octave_index, num_intervals, dog_images_in_octave, sigma, contrast_threshold, image_border_width)

                        localization_result = localizeExtremumViaQuadraticFit(i, j, image_index + 1, octave_index, num_intervals, dog_images_in_octave, sigma, contrast_threshold, image_border_width)

                        if localization_result is not None:

                        현지화_결과가 None이 아닌 경우:

                            keypoint, localized_image_index = localization_result

                            키포인트, 현지화_이미지_색인 = 현지화_결과

                            keypoints_with_orientations = computeKeypointsWithOrientations(keypoint, octave_index, gaussian_images[octave_index][localized_image_index])

                            키포인트_with_방향 = computeKeypointsWithOrientations(키포인트, 옥타브_인덱스, 가우시안_이미지[옥타브_인덱스][현지화된_이미지_인덱스])

                            for keypoint_with_orientation in keypoints_with_orientations:

                            키포인트_위드_오리엔테이션의 키포인트_위드_오리엔테이션에 대해:

                                keypoints.append(keypoint_with_orientation)

                                keypoints.append(keypoint_with_orientation)

    return keypoints

    키포인트 반환





def isPixelAnExtremum(first_subimage, second_subimage, third_subimage, threshold):

def isPixelAnExtremum(first_subimage, second_subimage, third_subimage, threshold):

    """Return True if the center element of the 3x3x3 input array is strictly greater than or less than all its neighbors, False otherwise

    """3x3x3 입력 배열의 중심 요소가 모든 이웃 요소보다 엄격하게 크거나 작으면 True를 반환하고, 그렇지 않으면 False를 반환합니다.

    """

    """

    center_pixel_value = second_subimage[1, 1]

    center_pixel_value = second_subimage[1, 1]

    if abs(center_pixel_value) > threshold:

    if abs(center_pixel_value) > 임계값:

        if center_pixel_value > 0:

        if center_pixel_value > 0:

            return all(center_pixel_value >= first_subimage) and \

            return all(center_pixel_value >= first_subimage) 및 \.

                   all(center_pixel_value >= third_subimage) and \

                   all(center_pixel_value >= third_subimage) 및 \.

                   all(center_pixel_value >= second_subimage[0, :]) and \

                   all(center_pixel_value >= second_subimage[0, :]) 및 \.

                   all(center_pixel_value >= second_subimage[2, :]) and \

                   all(center_pixel_value >= second_subimage[2, :]) 및 \.

                   center_pixel_value >= second_subimage[1, 0] and \

                   center_pixel_value >= second_subimage[1, 0] 및 \.

                   center_pixel_value >= second_subimage[1, 2]

                   center_pixel_value >= second_subimage[1, 2]

        elif center_pixel_value < 0:

        elif center_pixel_value < 0:

            return all(center_pixel_value <= first_subimage) and \

            return all(center_pixel_value <= first_subimage) 및 \.

                   all(center_pixel_value <= third_subimage) and \

                   all(center_pixel_value <= third_subimage) 및 \.

                   all(center_pixel_value <= second_subimage[0, :]) and \

                   all(center_pixel_value <= second_subimage[0, :]) 및 \.

                   all(center_pixel_value <= second_subimage[2, :]) and \

                   all(center_pixel_value <= second_subimage[2, :]) 및 \.

                   center_pixel_value <= second_subimage[1, 0] and \

                   center_pixel_value <= second_subimage[1, 0] 및 \.

                   center_pixel_value <= second_subimage[1, 2]

                   center_pixel_value <= second_subimage[1, 2]

    return False

    반환 False

```

```





This part’s easy. We just iterate through each layer, taking three successive images at a time. Remember that all images in a layer have the same size — only their amounts of blur differ. In each triplet of images, we look for pixels in the middle image that are greater than or less than all of their 26 neighbors: 8 neighbors in the middle image, 9 neighbors in the image below, and 9 neighbors in the image above. The function `isPixelAnExtremum()` performs this check. These are our maxima and minima (strictly speaking, they include saddle points because we include pixels that are equal in value to all their neighbors). When we’ve found an extremum, we localize its position at the subpixel level along all three dimensions (width, height, and scale) using `localizeExtremumViaQuadraticFit()`, explained in more detail below.

이 부분은 쉽습니다. 한 번에 세 개의 이미지를 연속적으로 촬영하면서 각 레이어를 반복하기만 하면 됩니다. 레이어의 모든 이미지는 크기가 같고 흐림의 양만 다르다는 점을 기억하세요. 각 세 개의 이미지에서 가운데 이미지의 이웃 8개, 아래 이미지의 이웃 9개, 위 이미지의 이웃 9개 등 26개의 이웃 이미지 모두보다 크거나 작은 픽셀을 찾습니다. 이 검사는 `isPixelAnExtremum()` 함수가 수행합니다. 이것이 최대값과 최소값입니다(엄밀히 말하면 모든 이웃 픽셀과 값이 같은 픽셀을 포함하므로 새들 포인트가 포함됩니다). 극값을 찾으면 아래에서 자세히 설명하는 `localizeExtremumViaQuadraticFit()`을 사용하여 세 가지 차원(폭, 높이, 배율)을 따라 서브픽셀 수준에서 위치를 찾습니다.





Our last step here is to compute orientations for each keypoint, which we’ll cover in Part 2 of this tutorial. There may be more than one orientation, so we create and append a new keypoint for each one.

마지막 단계는 각 키포인트의 방향을 계산하는 것으로, 이 튜토리얼의 2부에서 다룰 것입니다. 방향이 두 개 이상일 수 있으므로 각 방향에 대해 새 키포인트를 생성하고 추가합니다.





After localization, we check that the keypoint’s new pixel location has enough contrast. I’ll spare you the math because the SIFT paper explains this in good detail. In a nutshell, the ratio of eigenvalues of the 2D Hessian along the width and height of the keypoint’s image gives us contrast information.

현지화 후에는 키포인트의 새 픽셀 위치에 충분한 대비가 있는지 확인합니다. 이에 대한 자세한 설명은 SIFT 백서에 나와 있으므로 수식은 생략하겠습니다. 간단히 말해, 키포인트 이미지의 폭과 높이에 따른 2D 헤시안 고유값의 비율은 대비 정보를 제공합니다.





One last note: the keypoint’s position (`keypoint.pt`) is repeatedly doubled according to its layer so that it corresponds to coordinates in the base image. The `keypoint.octave` and `keypoint.size` (i.e., scale) attributes are defined according to the OpenCV implementation. These two will come in handy later.

마지막으로 키포인트의 위치(`keypoint.pt`)는 레이어에 따라 반복적으로 두 배가 되어 기본 이미지의 좌표와 일치하도록 합니다. '키포인트.옥타브' 및 '키포인트.크기'(즉, 스케일) 속성은 OpenCV 구현에 따라 정의됩니다. 이 두 가지는 나중에 유용하게 사용될 것입니다.





You can try plotting your keypoints over the base image to visualize the pixels SIFT finds interesting. I’ve done so below on box.png from the repo:

기본 이미지 위에 키포인트를 플로팅하여 SIFT가 흥미롭다고 생각하는 픽셀을 시각화해 볼 수 있습니다. 아래 리포지토리의 box.png에서 그렇게 해보았습니다:





<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_03.png" width="600px"></p>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_03.png" width="600px"></p>





<center>Our initial keypoints plotted over the base image.</center>

<center>기본 이미지 위에 그려진 초기 키포인트 </center>





# Localizing Extrema

# 극한값 현지화하기

```

```

def localizeExtremumViaQuadraticFit(i, j, image_index, octave_index, num_intervals, dog_images_in_octave, sigma, contrast_threshold, image_border_width, eigenvalue_ratio=10, num_attempts_until_convergence=5):

def localizeExtremumViaQuadraticFit(i, j, image_index, octave_index, num_intervals, dog_images_in_octave, sigma, contrast_threshold, image_border_width, eigenvalue_ratio=10, num_attempts_until_convergence=5):

    """Iteratively refine pixel positions of scale-space extrema via quadratic fit around each extremum's neighbors

    """각 극값의 이웃을 중심으로 이차 적합을 통해 스케일 공간 극값의 픽셀 위치를 반복적으로 다듬습니다.

    """

    """

    logger.debug('Localizing scale-space extrema...')

    logger.debug('스케일-공간 극한값 현지화...')

    extremum_is_outside_image = False

    extremum_is_outside_image = False

    image_shape = dog_images_in_octave[0].shape

    image_shape = dog_images_in_octave[0].shape

    for attempt_index in range(num_attempts_until_convergence):

    시도_인덱스에 대해 범위(num_attempts_until_convergence):

        # need to convert from uint8 to float32 to compute derivatives and need to rescale pixel values to [0, 1] to apply Lowe's thresholds

        # 파생값을 계산하기 위해 uint8에서 float32로 변환해야 하며 Lowe의 임계값을 적용하려면 픽셀 값을 [0, 1]로 리스케일해야 합니다.

        first_image, second_image, third_image = dog_images_in_octave[image_index-1:image_index+2]

        first_image, second_image, third_image = dog_images_in_octave[image_index-1:image_index+2]

        pixel_cube = stack([first_image[i-1:i+2, j-1:j+2],

        픽셀 큐브 = 스택([first_image[i-1:i+2, j-1:j+2],

                            second_image[i-1:i+2, j-1:j+2],

                            second_image[i-1:i+2, j-1:j+2],

                            third_image[i-1:i+2, j-1:j+2]]).astype('float32') / 255.

                            third_image[i-1:i+2, j-1:j+2]]).astype('float32') / 255.

        gradient = computeGradientAtCenterPixel(pixel_cube)

        그라디언트 = 중심 픽셀에서 그라디언트 계산(픽셀_큐브)

        hessian = computeHessianAtCenterPixel(pixel_cube)

        헤시안 = 계산헤시안앳센터픽셀(픽셀_큐브)

        extremum_update = -lstsq(hessian, gradient, rcond=None)[0]

        extremum_update = -lstsq(hessian, gradient, rcond=None)[0]

        if abs(extremum_update[0]) < 0.5 and abs(extremum_update[1]) < 0.5 and abs(extremum_update[2]) < 0.5:

        abs(extremum_update[0]) < 0.5 및 abs(extremum_update[1]) < 0.5 및 abs(extremum_update[2]) < 0.5:

            break

            break

        j += int(round(extremum_update[0]))

        j += int(round(extremum_update[0]))

        i += int(round(extremum_update[1]))

        i += int(round(extremum_update[1]))

        image_index += int(round(extremum_update[2]))

        image_index += int(round(extremum_update[2]))

        # make sure the new pixel_cube will lie entirely within the image

        # 새 픽셀 큐브가 이미지 안에 완전히 위치하는지 확인합니다.

        if i < image_border_width or i >= image_shape[0] - image_border_width or j < image_border_width or j >= image_shape[1] - image_border_width or image_index < 1 or image_index > num_intervals:

        if i < image_border_width 또는 i >= image_shape[0] - image_border_width 또는 j < image_border_width 또는 j >= image_shape[1] - image_border_width 또는 image_index < 1 또는 image_index > num_intervals:

            extremum_is_outside_image = True

            extremum_is_outside_image = True

            break

            break

    if extremum_is_outside_image:

    극단값이 이미지 외부에 있는 경우:

        logger.debug('Updated extremum moved outside of image before reaching convergence. Skipping...')

        logger.debug('업데이트된 극값이 수렴에 도달하기 전에 이미지 외부로 이동했습니다. 스키핑...')

        return None

        반환 없음

    if attempt_index >= num_attempts_until_convergence - 1:

    if attempt_index >= num_attempts_until_convergence - 1:

        logger.debug('Exceeded maximum number of attempts without reaching convergence for this extremum. Skipping...')

        logger.debug('이 극한값에 대한 수렴에 도달하지 않고 최대 시도 횟수를 초과했습니다. Skipping...')

        return None

        반환 없음

    functionValueAtUpdatedExtremum = pixel_cube[1, 1, 1] + 0.5 * dot(gradient, extremum_update)

    functionValueAtUpdatedExtremum = pixel_cube[1, 1, 1] + 0.5 * dot(gradient, extremum_update)

    if abs(functionValueAtUpdatedExtremum) * num_intervals >= contrast_threshold:

    abs(functionValueAtUpdatedExtremum) * num_intervals >= contrast_threshold:

        xy_hessian = hessian[:2, :2]

        xy_hessian = hessian[:2, :2]

        xy_hessian_trace = trace(xy_hessian)

        xy_hessian_trace = trace(xy_hessian)

        xy_hessian_det = det(xy_hessian)

        xy_hessian_det = det(xy_hessian)

        if xy_hessian_det > 0 and eigenvalue_ratio * (xy_hessian_trace ** 2) < ((eigenvalue_ratio + 1) ** 2) * xy_hessian_det:

        만약 xy_hessian_det > 0이고 고유값_비율 * (xy_hessian_trace ** 2) < ((고유값_비율 + 1) ** 2) * xy_hessian_det:

            # Contrast check passed -- construct and return OpenCV KeyPoint object

            # 대비 검사 통과 -- OpenCV KeyPoint 객체를 생성하고 반환합니다.

            keypoint = KeyPoint()

            keypoint = KeyPoint()

            keypoint.pt = ((j + extremum_update[0]) * (2 ** octave_index), (i + extremum_update[1]) * (2 ** octave_index))

            keypoint.pt = ((j + extremum_update[0]) * (2 ** 옥타브_인덱스), (i + extremum_update[1]) * (2 ** 옥타브_인덱스))

            keypoint.octave = octave_index + image_index * (2 ** 8) + int(round((extremum_update[2] + 0.5) * 255)) * (2 ** 16)

            keypoint.octave = octave_index + image_index * (2 ** 8) + int(round((extremum_update[2] + 0.5) * 255)) * (2 ** 16)

            keypoint.size = sigma * (2 ** ((image_index + extremum_update[2]) / float32(num_intervals))) * (2 ** (octave_index + 1))  # octave_index + 1 because the input image was doubled

            keypoint.size = 시그마 * (2 ** ((image_index + extremum_update[2]) / float32(num_intervals)))) * (2 ** (옥타브_인덱스 + 1))  입력 이미지가 두 배가 되었기 때문에 # 옥타브_인덱스 + 1

            keypoint.response = abs(functionValueAtUpdatedExtremum)

            keypoint.response = abs(functionValueAtUpdatedExtremum)

            return keypoint, image_index

            반환 키포인트, 이미지_인덱스

    return None

    반환 없음





def computeGradientAtCenterPixel(pixel_array):

def computeGradientAtCenterPixel(pixel_array):

    """Approximate gradient at center pixel [1, 1, 1] of 3x3x3 array using central difference formula of order O(h^2), where h is the step size

    """3x3x3 배열의 중심 픽셀 [1, 1, 1]에서 O(h^2) 차수의 중심 차등 공식을 사용하여 대략적인 그라데이션, 여기서 h는 단계 크기입니다.

    """

    """

    # With step size h, the central difference formula of order O(h^2) for f'(x) is (f(x + h) - f(x - h)) / (2 * h)

    # 스텝 크기 h의 경우, f'(x)에 대한 O(h^2) 차수의 중심 차분 공식은 (f(x + h) - f(x - h)) / (2 * h)입니다.

    # Here h = 1, so the formula simplifies to f'(x) = (f(x + 1) - f(x - 1)) / 2

    # 여기서 h = 1이므로 공식은 f'(x) = (f(x + 1) - f(x - 1)) / 2로 단순화됩니다.

    # NOTE: x corresponds to second array axis, y corresponds to first array axis, and s (scale) corresponds to third array axis

    # 참고: x는 두 번째 배열 축에 해당하고, y는 첫 번째 배열 축에 해당하며, s(스케일)는 세 번째 배열 축에 해당합니다.

    dx = 0.5 * (pixel_array[1, 1, 2] - pixel_array[1, 1, 0])

    dx = 0.5 * (픽셀 배열[1, 1, 2] - 픽셀 배열[1, 1, 0])

    dy = 0.5 * (pixel_array[1, 2, 1] - pixel_array[1, 0, 1])

    dy = 0.5 * (픽셀 배열[1, 2, 1] - 픽셀 배열[1, 0, 1])

    ds = 0.5 * (pixel_array[2, 1, 1] - pixel_array[0, 1, 1])

    ds = 0.5 * (pixel_array[2, 1, 1] - pixel_array[0, 1, 1])

    return array([dx, dy, ds])

    return array([dx, dy, ds])





def computeHessianAtCenterPixel(pixel_array):

def computeHessianAtCenterPixel(pixel_array):

    """Approximate Hessian at center pixel [1, 1, 1] of 3x3x3 array using central difference formula of order O(h^2), where h is the step size

    """3x3x3 배열의 중심 픽셀 [1, 1, 1]에서 O(h^2) 차수의 중심 차분 공식을 사용하여 헤시안 근사값을 구합니다. 여기서 h는 단계 크기입니다.

    """

    """

    # With step size h, the central difference formula of order O(h^2) for f''(x) is (f(x + h) - 2 * f(x) + f(x - h)) / (h ^ 2)

    # 스텝 크기 h의 경우, f''(x)에 대한 O(h^2) 차수의 중심 차분 공식은 (f(x + h) - 2 * f(x) + f(x - h)) / (h ^ 2) 입니다.

    # Here h = 1, so the formula simplifies to f''(x) = f(x + 1) - 2 * f(x) + f(x - 1)

    # 여기서 h = 1이므로 공식은 f''(x) = f(x + 1) - 2 * f(x) + f(x - 1)로 단순화됩니다.

    # With step size h, the central difference formula of order O(h^2) for (d^2) f(x, y) / (dx dy) = (f(x + h, y + h) - f(x + h, y - h) - f(x - h, y + h) + f(x - h, y - h)) / (4 * h ^ 2)

    # 스텝 크기 h의 경우, (d^2) f(x, y) / (dx dy)에 대한 O(h^2) 차수의 중심 차분 공식은 (f(x + h, y + h) - f(x + h, y - h) - f(x - h, y + h) + f(x - h, y - h)) / (4 * h ^ 2) 입니다.

    # Here h = 1, so the formula simplifies to (d^2) f(x, y) / (dx dy) = (f(x + 1, y + 1) - f(x + 1, y - 1) - f(x - 1, y + 1) + f(x - 1, y - 1)) / 4

    # 여기서 h = 1이므로 공식은 (d^2) f(x, y) / (dx dy) = (f(x + 1, y + 1) - f(x + 1, y - 1) - f(x - 1, y + 1) + f(x - 1, y - 1)) / 4로 단순화됩니다.

    # NOTE: x corresponds to second array axis, y corresponds to first array axis, and s (scale) corresponds to third array axis

    # 참고: x는 두 번째 배열 축, y는 첫 번째 배열 축, s(스케일)는 세 번째 배열 축에 해당합니다.

    center_pixel_value = pixel_array[1, 1, 1]

    center_pixel_value = pixel_array[1, 1, 1]

    dxx = pixel_array[1, 1, 2] - 2 * center_pixel_value + pixel_array[1, 1, 0]

    dxx = pixel_array[1, 1, 2] - 2 * center_pixel_value + pixel_array[1, 1, 0]

    dyy = pixel_array[1, 2, 1] - 2 * center_pixel_value + pixel_array[1, 0, 1]

    dyy = pixel_array[1, 2, 1] - 2 * center_pixel_value + pixel_array[1, 0, 1]

    dss = pixel_array[2, 1, 1] - 2 * center_pixel_value + pixel_array[0, 1, 1]

    dss = pixel_array[2, 1, 1] - 2 * center_pixel_value + pixel_array[0, 1, 1]

    dxy = 0.25 * (pixel_array[1, 2, 2] - pixel_array[1, 2, 0] - pixel_array[1, 0, 2] + pixel_array[1, 0, 0])

    dxy = 0.25 * (pixel_array[1, 2, 2] - pixel_array[1, 2, 0] - pixel_array[1, 0, 2] + pixel_array[1, 0, 0])

    dxs = 0.25 * (pixel_array[2, 1, 2] - pixel_array[2, 1, 0] - pixel_array[0, 1, 2] + pixel_array[0, 1, 0])

    dxs = 0.25 * (pixel_array[2, 1, 2] - pixel_array[2, 1, 0] - pixel_array[0, 1, 2] + pixel_array[0, 1, 0])

    dys = 0.25 * (pixel_array[2, 2, 1] - pixel_array[2, 0, 1] - pixel_array[0, 2, 1] + pixel_array[0, 0, 1])

    dys = 0.25 * (pixel_array[2, 2, 1] - pixel_array[2, 0, 1] - pixel_array[0, 2, 1] + pixel_array[0, 0, 1])

    return array([[dxx, dxy, dxs], 

    return array([[dxx, dxy, dxs],

                  [dxy, dyy, dys],

                  [dxy, dyy, dys],

                  [dxs, dys, dss]])

                  [dxs, dys, dss]]))

```

```





The code to localize a keypoint may look involved, but it’s actually pretty straightforward. It implements verbatim the localization procedure described in the original SIFT paper. We fit a quadratic model to the input keypoint pixel and all 26 of its neighboring pixels (we call this a `pixel_cube`). We update the keypoint’s position with the subpixel-accurate extremum estimated from this model. We iterate at most 5 times until the next update moves the keypoint less than 0.5 in any of the three directions. This means the quadratic model has converged to one pixel location. The two helper functions `computeGradientAtCenterPixel()` and `computeHessianAtCenterPixel()` implement second-order central finite difference approximations of the gradients and hessians in all three dimensions. The key takeaway is that ordinary quadratic interpolation that you may have done in calculus class won’t work well here because we’re using a uniform mesh (a grid of evenly spaced pixels). Finite difference approximations take into account this discretization to produce more accurate extrema estimates.

키포인트를 지역화하는 코드는 복잡해 보이지만 실제로는 매우 간단합니다. 이 코드는 원본 SIFT 문서에 설명된 현지화 절차를 그대로 구현합니다. 입력 키포인트 픽셀과 그 주변 픽셀 26개(이를 '픽셀_큐브'라고 부릅니다)에 이차 모델을 맞춥니다. 이 모델에서 추정된 서브픽셀 정확도의 극한값으로 키포인트의 위치를 업데이트합니다. 다음 업데이트에서 키포인트가 세 방향 중 하나에서 0.5보다 작게 움직일 때까지 최대 5회 반복합니다. 이는 이차 모델이 한 픽셀 위치로 수렴했음을 의미합니다. 두 개의 도우미 함수 `computeGradientAtCenterPixel()`과 `computeHessianAtCenterPixel()`은 모든 3차원에서 그래디언트와 헤시안의 2차 중심 유한차분 근사치를 구현합니다. 여기서 중요한 점은 미적분 시간에 배웠던 일반적인 이차 보간은 균일한 메쉬(픽셀 간격이 균일한 격자)를 사용하기 때문에 잘 작동하지 않는다는 것입니다. 유한차 근사치는 이러한 이산화를 고려하여 보다 정확한 극한값 추정치를 생성합니다.





Now we’ve found our keypoints and have accurately localized them. We’ve gotten far, but there are two big tasks left: computing orientations and generating descriptors.

이제 키포인트를 찾았고 정확하게 위치를 파악했습니다. 여기까지 왔지만 방향 계산과 설명자 생성이라는 두 가지 큰 작업이 남아 있습니다.





-----

-----





# Recap

# 요약

So far we’ve generated a series of scales at which to blur our input image, which we used to generate a Gaussian-blurred image pyramid and a difference-of-Gaussians image pyramid. We found our keypoint positions by looking for local maxima and minima along the image width, height, and scale dimensions. We localized these keypoint positions to subpixel accuracy with the help of quadratic interpolation.

지금까지 가우스 블러 이미지 피라미드와 가우시안 차 이미지 피라미드를 생성하는 데 사용한 입력 이미지를 흐리게 하는 일련의 스케일을 생성했습니다. 이미지 너비, 높이, 배율 차원을 따라 로컬 최대값과 최소값을 찾아 키포인트 위치를 찾았습니다. 이 키포인트 위치를 이차 보간을 통해 서브픽셀 정확도로 현지화했습니다.





What’s left? We need to compute orientations for each of our keypoints. We’ll likely also produce keypoints with identical positions but different orientations. Then we’ll sort our finished keypoints and remove duplicates. Finally, we’ll generate descriptor vectors for each of our keypoints, which allow keypoints to be identified and compared.

남은 작업은 무엇인가요? 이제 각 키포인트의 방향을 계산해야 합니다. 위치는 같지만 방향이 다른 키포인트도 생성할 수 있습니다. 그런 다음 완성된 키포인트를 정렬하고 중복을 제거합니다. 마지막으로 각 키포인트에 대한 설명자 벡터를 생성하여 키포인트를 식별하고 비교할 수 있도록 합니다.





At the end of this tutorial, we’ll see SIFT in action with a template matching demo.

이 튜토리얼의 마지막에는 템플릿 매칭 데모를 통해 SIFT가 실제로 작동하는 모습을 보여드리겠습니다.





Let’s get started.

시작해 보겠습니다.





# Keypoint Orientations

# 키포인트 방향





```

```

def computeKeypointsWithOrientations(keypoint, octave_index, gaussian_image, radius_factor=3, num_bins=36, peak_ratio=0.8, scale_factor=1.5):

def computeKeypointsWithOrientations(키포인트, 옥타브_인덱스, 가우시안_이미지, 반경_인자=3, 넘버빈=36, 피크_비율=0.8, 스케일_인자=1.5):

    """Compute orientations for each keypoint

    """각 키포인트의 방향을 계산합니다.

    """

    """

    logger.debug('Computing keypoint orientations...')

    logger.debug('키포인트 방향 계산 중...')

    keypoints_with_orientations = []

    keypoints_with_orientations = []

    image_shape = gaussian_image.shape

    image_shape = 가우스_이미지.모양





    scale = scale_factor * keypoint.size / float32(2 ** (octave_index + 1))  # compare with keypoint.size computation in localizeExtremumViaQuadraticFit()

    scale = scale_factor * 키포인트_크기 / float32(2 ** (옥타브_인덱스 + 1))  # 현지화ExtremumViaQuadraticFit()의 keypoint.size 계산과 비교합니다.

    radius = int(round(radius_factor * scale))

    radius = int(round(radius_factor * scale))

    weight_factor = -0.5 / (scale ** 2)

    weight_factor = -0.5 / (스케일 ** 2)

    raw_histogram = zeros(num_bins)

    raw_histogram = 0(num_bins)

    smooth_histogram = zeros(num_bins)

    smooth_histogram = zeros(num_bins)





    for i in range(-radius, radius + 1):

    범위(-반경, 반경 + 1)의 i에 대해:

        region_y = int(round(keypoint.pt[1] / float32(2 ** octave_index))) + i

        region_y = int(round(keypoint.pt[1] / float32(2 ** 옥타브_인덱스))) + i

        if region_y > 0 and region_y < image_shape[0] - 1:

        region_y > 0이고 region_y < image_shape[0] - 1인 경우:

            for j in range(-radius, radius + 1):

            범위(-반경, 반경 + 1)의 j에 대해:

                region_x = int(round(keypoint.pt[0] / float32(2 ** octave_index))) + j

                region_x = int(round(keypoint.pt[0] / float32(2 ** octave_index))) + j

                if region_x > 0 and region_x < image_shape[1] - 1:

                region_x > 0이고 region_x < image_shape[1] - 1인 경우:

                    dx = gaussian_image[region_y, region_x + 1] - gaussian_image[region_y, region_x - 1]

                    dx = 가우시안_이미지[region_y, region_x + 1] - 가우시안_이미지[region_y, region_x - 1]

                    dy = gaussian_image[region_y - 1, region_x] - gaussian_image[region_y + 1, region_x]

                    dy = 가우시안_이미지[region_y - 1, region_x] - 가우시안_이미지[region_y + 1, region_x]

                    gradient_magnitude = sqrt(dx * dx + dy * dy)

                    gradient_magnitude = sqrt(dx * dx + dy * dy)

                    gradient_orientation = rad2deg(arctan2(dy, dx))

                    gradient_orientation = rad2deg(arctan2(dy, dx))

                    weight = exp(weight_factor * (i ** 2 + j ** 2))  # constant in front of exponential can be dropped because we will find peaks later

                    weight = exp(weight_factor * (i ** 2 + j ** 2))  # 지수 앞의 상수는 나중에 피크를 찾을 것이므로 삭제할 수 있습니다.

                    histogram_index = int(round(gradient_orientation * num_bins / 360.))

                    히스토그램_인덱스 = int(라운드(그라데이션_방향 * num_bins / 360.))

                    raw_histogram[histogram_index % num_bins] += weight * gradient_magnitude

                    raw_histogram[histogram_index % num_bins] += weight * gradient_magnitude





    for n in range(num_bins):

    범위(num_bins)의 n에 대해:

        smooth_histogram[n] = (6 * raw_histogram[n] + 4 * (raw_histogram[n - 1] + raw_histogram[(n + 1) % num_bins]) + raw_histogram[n - 2] + raw_histogram[(n + 2) % num_bins]) / 16.

        smooth_histogram[n] = (6 * raw_histogram[n] + 4 * (raw_histogram[n - 1] + raw_histogram[(n + 1) % num_bins]) + raw_histogram[n - 2] + raw_histogram[(n + 2) % num_bins]) / 16.

    orientation_max = max(smooth_histogram)

    orientation_max = max(smooth_histogram)

    orientation_peaks = where(logical_and(smooth_histogram > roll(smooth_histogram, 1), smooth_histogram > roll(smooth_histogram, -1)))[0]

    orientation_peaks = where(logical_and(smooth_histogram > roll(smooth_histogram, 1), smooth_histogram > roll(smooth_histogram, -1))[0]

    for peak_index in orientation_peaks:

    오리엔테이션_피크의 peak_index에 대해:

        peak_value = smooth_histogram[peak_index]

        peak_value = smooth_histogram[peak_index]

        if peak_value >= peak_ratio * orientation_max:

        peak_value >= peak_ratio * orientation_max:

            # Quadratic peak interpolation

            이차 피크 보간: # 이차 피크 보간

            # The interpolation update is given by equation (6.30) in https://ccrma.stanford.edu/~jos/sasp/Quadratic_Interpolation_Spectral_Peaks.html

            # 보간 업데이트는 https://ccrma.stanford.edu/~jos/sasp/Quadratic_Interpolation_Spectral_Peaks.html의 방정식 (6.30)에 의해 제공됩니다.

            left_value = smooth_histogram[(peak_index - 1) % num_bins]

            left_value = smooth_histogram[(peak_index - 1) % num_bins]

            right_value = smooth_histogram[(peak_index + 1) % num_bins]

            right_value = smooth_histogram[(peak_index + 1) % num_bins]

            interpolated_peak_index = (peak_index + 0.5 * (left_value - right_value) / (left_value - 2 * peak_value + right_value)) % num_bins

            보간된_피크_인덱스 = (peak_index + 0.5 * (왼쪽_값 - 오른쪽_값) / (왼쪽_값 - 2 * 피크_값 + 오른쪽_값)) % num_bins

            orientation = 360. - interpolated_peak_index * 360. / num_bins

            방향 = 360. - 보간된_피크_인덱스 * 360. / num_bins

            if abs(orientation - 360.) < float_tolerance:

            abs(orientation - 360.) < float_tolerance:

                orientation = 0

                orientation = 0

            new_keypoint = KeyPoint(*keypoint.pt, keypoint.size, orientation, keypoint.response, keypoint.octave)

            new_keypoint = 키포인트(*keypoint.pt, 키포인트.크기, 방향, 키포인트.응답, 키포인트.옥타브)

            keypoints_with_orientations.append(new_keypoint)

            keypoints_with_orientations.append(new_keypoint)

    return keypoints_with_orientations

    반환 키포인트_위드_오리엔테이션

```

```





Take a look at `computeKeypointsWithOrientations()`. The goal here is to create a histogram of gradients for pixels around the keypoint’s neighborhood. Note that we use a square neighborhood here, as the OpenCV implementation does. One detail to note is the `radius_factor`, which is 3 by default. This means the neighborhood will cover pixels within `3 * scale` of each keypoint, where `scale` is the standard deviation associated with the Gaussian weighting. Remember that 99.7% of the probability density of a Gaussian distribution lies inside three standard deviations, which in this case means 99.7% of the weight lies within `3 * scale` pixels of the keypoint.

계산 키포인트와 방향()`을 살펴보세요. 여기서 목표는 키포인트의 주변 픽셀에 대한 그라데이션 히스토그램을 만드는 것입니다. 여기서는 OpenCV 구현에서와 같이 정사각형 이웃을 사용한다는 점에 유의하세요. 한 가지 주목해야 할 세부 사항은 기본적으로 3인 '반경 계수(radius_factor)'입니다. 이는 이웃이 각 키포인트의 '3 * 스케일' 이내의 픽셀을 포함한다는 의미이며, 여기서 '스케일'은 가우스 가중치와 관련된 표준 편차입니다. 가우스 분포의 확률 밀도의 99.7%는 3개의 표준 편차 내에 있으며, 이 경우 가중치의 99.7%가 키포인트의 `3 * 스케일` 픽셀 내에 있다는 것을 의미합니다.





Next, we compute the magnitude and orientation of the 2D gradient at each pixel in this neighborhood. We create a 36-bin histogram for the orientations — 10 degrees per bin. The orientation of a particular pixel tells us which histogram bin to choose, but the actual value we place in that bin is that pixel’s gradient magnitude with a Gaussian weighting. This makes pixels farther from the keypoint have less of an influence on the histogram. We repeat this procedure for all pixels in the neighborhood, accumulating our results into the same 36-bin histogram.

다음으로, 이 영역의 각 픽셀에서 2D 그라데이션의 크기와 방향을 계산합니다. 방향에 대해 36개의 빈 히스토그램(빈당 10도)을 생성합니다. 특정 픽셀의 방향은 어떤 히스토그램 구간을 선택할지 알려주지만, 해당 구간에 배치하는 실제 값은 가우시안 가중치를 적용한 해당 픽셀의 그라데이션 크기입니다. 이렇게 하면 키포인트에서 멀리 떨어진 픽셀이 히스토그램에 미치는 영향이 줄어듭니다. 이웃의 모든 픽셀에 대해 이 절차를 반복하여 결과를 동일한 36-빈 히스토그램으로 누적합니다.





When we’re all done, we smooth the histogram. The smoothing coefficients correspond to a 5-point Gaussian filter, and you can find a neat explanation of how to derive these coefficients from Pascal’s triangle.

모든 작업이 완료되면 히스토그램을 평활화합니다. 평활화 계수는 5점 가우스 필터에 해당하며, 파스칼의 삼각형에서 이러한 계수를 도출하는 방법에 대한 깔끔한 설명을 찾을 수 있습니다.





To help us visualize what’s going on, I’ve plotted the raw histogram and smoothed histogram below for an actual keypoint produced when our code is applied to the template image `box.png` (found in the Github).

무슨 일이 일어나고 있는지 시각화하기 위해 템플릿 이미지 'box.png'에 코드를 적용했을 때 생성되는 실제 키포인트에 대한 원시 히스토그램과 평활화된 히스토그램을 아래에 표시했습니다(Github에서 찾을 수 있음).









<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_04.png" width="700px"></p>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_04.png" width="700px"></p>





<center>The raw orientation histogram.</center>

<center>원시 방향 히스토그램입니다.</center>





<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_05.png" width="700px"></p>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_05.png" width="700px"></p>





<center>The Gaussian-smoothed orientation histogram with peaks in red.</center>

<center>빨간색 피크가 있는 가우스 평활 방향 히스토그램.</center>





Now we look for peaks in this histogram that lie above a threshold specified in the SIFT paper. We localize each peak using quadratic interpolation — finite differences again!  We create a separate keypoint for each peak, and these keypoints will be identical except for their orientation attributes. As stated in the SIFT paper, these additional keypoints significantly contribute to detection stability when used in real applications.

이제 이 히스토그램에서 SIFT 문서에 지정된 임계값 위에 있는 피크를 찾습니다. 다시 유한 차분인 이차 보간을 사용하여 각 피크의 위치를 찾습니다!  각 피크에 대해 별도의 키포인트를 생성하며, 이러한 키포인트는 방향 속성을 제외하고는 동일합니다. SIFT 논문에서 언급했듯이, 이러한 추가 키포인트는 실제 애플리케이션에서 사용할 때 감지 안정성에 크게 기여합니다.





# Cleaning Up Keypoints

# 키포인트 정리하기





```

```

def compareKeypoints(keypoint1, keypoint2):

def compareKeypoints(keypoint1, keypoint2):

    """Return True if keypoint1 is less than keypoint2

    """키포인트1이 키포인트2보다 작으면 True를 반환합니다.

    """

    """

    if keypoint1.pt[0] != keypoint2.pt[0]:

    if keypoint1.pt[0] != keypoint2.pt[0]:

        return keypoint1.pt[0] - keypoint2.pt[0]

        반환 키포인트1.pt[0] - 키포인트2.pt[0]

    if keypoint1.pt[1] != keypoint2.pt[1]:

    if keypoint1.pt[1] != keypoint2.pt[1]:

        return keypoint1.pt[1] - keypoint2.pt[1]

        반환 keypoint1.pt[1] - keypoint2.pt[1]

    if keypoint1.size != keypoint2.size:

    if keypoint1.size != keypoint2.size:

        return keypoint2.size - keypoint1.size

        반환 키포인트2.크기 - 키포인트1.크기

    if keypoint1.angle != keypoint2.angle:

    if keypoint1.angle != keypoint2.angle:

        return keypoint1.angle - keypoint2.angle

        반환 키포인트1.각도 - 키포인트2.각도

    if keypoint1.response != keypoint2.response:

    만약 키포인트1.응답 != 키포인트2.응답:

        return keypoint2.response - keypoint1.response

        반환 키포인트2.응답 - 키포인트1.응답

    if keypoint1.octave != keypoint2.octave:

    if keypoint1.옥타브 != keypoint2.옥타브:

        return keypoint2.octave - keypoint1.octave

        반환 키포인트2.옥타브 - 키포인트1.옥타브

    return keypoint2.class_id - keypoint1.class_id

    반환 키포인트2.class_id - 키포인트1.class_id





def removeDuplicateKeypoints(keypoints):

def removeDuplicateKeypoints(키포인트):

    """Sort keypoints and remove duplicate keypoints

    """키포인트를 정렬하고 중복된 키포인트를 제거합니다.

    """

    """

    if len(keypoints) < 2:

    if len(keypoints) < 2:

        return keypoints

        반환 키포인트





    keypoints.sort(key=cmp_to_key(compareKeypoints))

    keypoints.sort(key=cmp_to_key(compareKeypoints))

    unique_keypoints = [keypoints[0]]

    unique_keypoints = [keypoints[0]]





    for next_keypoint in keypoints[1:]:

    keypoints[1:]의 next_keypoint에 대해:

        last_unique_keypoint = unique_keypoints[-1]

        last_unique_keypoint = unique_keypoints[-1]

        if last_unique_keypoint.pt[0] != next_keypoint.pt[0] or \

        if last_unique_keypoint.pt[0] != next_keypoint.pt[0] 또는 \.

           last_unique_keypoint.pt[1] != next_keypoint.pt[1] or \

           last_unique_keypoint.pt[1] != next_keypoint.pt[1] 또는 \.

           last_unique_keypoint.size != next_keypoint.size or \

           last_unique_keypoint.size != next_keypoint.size 또는 \.

           last_unique_keypoint.angle != next_keypoint.angle:

           last_unique_keypoint.angle != next_keypoint.angle:

            unique_keypoints.append(next_keypoint)

            unique_keypoints.append(next_keypoint)

    return unique_keypoints

    고유 키포인트 반환





def convertKeypointsToInputImageSize(keypoints):

def convertKeypointsToInputImageSize(keypoints):

    """Convert keypoint point, size, and octave to input image size

    """키포인트 포인트, 크기, 옥타브를 입력 이미지 크기로 변환합니다.

    """

    """

    converted_keypoints = []

    converted_keypoints = []

    for keypoint in keypoints:

    키포인트의 키포인트에 대해:

        keypoint.pt = tuple(0.5 * array(keypoint.pt))

        keypoint.pt = 튜플(0.5 * array(keypoint.pt))

        keypoint.size *= 0.5

        keypoint.size *= 0.5

        keypoint.octave = (keypoint.octave & ~255) | ((keypoint.octave - 1) & 255)

        keypoint.옥타브 = (keypoint.옥타브 & ~255) | ((keypoint.옥타브 - 1) & 255)

        converted_keypoints.append(keypoint)

        converted_keypoints.append(키포인트)

    return converted_keypoints

    반환 converted_keypoints

```

```





Now we’ve found all our keypoints. Before moving on to descriptor generation, we need to do some cleanup.

이제 모든 키포인트를 찾았습니다. 디스크립터 생성으로 넘어가기 전에 몇 가지 정리를 해야 합니다.





We sort and remove duplicates with `removeDuplicateKeypoints()`. The comparison function we use for keypoints is identical to that implemented in OpenCV.

removeDuplicateKeypoints()`로 중복을 정렬하고 제거합니다. 키포인트에 사용하는 비교 함수는 OpenCV에서 구현된 것과 동일합니다.





We also need to convert our keypoints from base image coordinates to input image coordinates, which we can accomplish by simply halving the relevant attributes.

또한 키포인트를 기본 이미지 좌표에서 입력 이미지 좌표로 변환해야 하는데, 이는 관련 속성을 절반으로 줄이면 됩니다.





# Generating Descriptors

# 설명자 생성하기





```

```

def unpackOctave(keypoint):

def unpackOctave(keypoint):

    """Compute octave, layer, and scale from a keypoint

    """키포인트에서 옥타브, 레이어, 스케일을 계산합니다.

    """

    """

    octave = keypoint.octave & 255

    옥타브 = 키포인트.옥타브 & 255

    layer = (keypoint.octave >> 8) & 255

    layer = (keypoint.octave >> 8) & 255

    if octave >= 128:

    옥타브 >= 128이면

        octave = octave | -128

        옥타브 = 옥타브 | -128

    scale = 1 / float32(1 << octave) if octave >= 0 else float32(1 << -octave)

    scale = 1 / float32(1 << 옥타브) if 옥타브 >= 0 else float32(1 << -옥타브)

    return octave, layer, scale

    옥타브, 레이어, 스케일 반환





def generateDescriptors(keypoints, gaussian_images, window_width=4, num_bins=8, scale_multiplier=3, descriptor_max_value=0.2):

def generateDescriptors(keypoints, gaussian_images, window_width=4, num_bins=8, scale_multiplier=3, descriptor_max_value=0.2):

    """Generate descriptors for each keypoint

    """각 키포인트에 대한 설명자 생성

    """

    """

    logger.debug('Generating descriptors...')

    logger.debug('디스크립터 생성 중...')

    descriptors = []

    descriptors = []





    for keypoint in keypoints:

    키포인트의 키포인트에 대해:

        octave, layer, scale = unpackOctave(keypoint)

        옥타브, 레이어, 스케일 = unpackOctave(키포인트)

        gaussian_image = gaussian_images[octave + 1, layer]

        가우시안_이미지 = 가우시안_이미지[옥타브 + 1, 레이어]

        num_rows, num_cols = gaussian_image.shape

        num_rows, num_cols = 가우시안_이미지.모양

        point = round(scale * array(keypoint.pt)).astype('int')

        point = round(scale * array(keypoint.pt)).astype('int')

        bins_per_degree = num_bins / 360.

        bins_per_degree = num_bins / 360.

        angle = 360. - keypoint.angle

        각도 = 360. - 키포인트.각도

        cos_angle = cos(deg2rad(angle))

        cos_angle = cos(deg2rad(angle))

        sin_angle = sin(deg2rad(angle))

        sin_angle = sin(deg2rad(angle))

        weight_multiplier = -0.5 / ((0.5 * window_width) ** 2)

        weight_multiplier = -0.5 / ((0.5 * window_width) ** 2)

        row_bin_list = []

        row_bin_list = []

        col_bin_list = []

        COL_BIN_LIST = []

        magnitude_list = []

        magnitude_list = []

        orientation_bin_list = []

        orientation_bin_list = []

        histogram_tensor = zeros((window_width + 2, window_width + 2, num_bins))   # first two dimensions are increased by 2 to account for border effects

        히스토그램_텐서 = 0((window_width + 2, window_width + 2, num_bins))   # 테두리 효과를 고려하기 위해 처음 두 차원이 2씩 증가합니다.





        # Descriptor window size (described by half_width) follows OpenCV convention

        # 설명자 창 크기(반_폭으로 설명됨)는 OpenCV 규칙을 따릅니다.

        hist_width = scale_multiplier * 0.5 * scale * keypoint.size

        hist_width = scale_multiplier * 0.5 * scale * keypoint.size

        half_width = int(round(hist_width * sqrt(2) * (window_width + 1) * 0.5))   # sqrt(2) corresponds to diagonal length of a pixel

        half_width = int(round(hist_width * sqrt(2) * (window_width + 1) * 0.5))   # sqrt(2)는 픽셀의 대각선 길이에 해당합니다.

        half_width = int(min(half_width, sqrt(num_rows ** 2 + num_cols ** 2)))     # ensure half_width lies within image

        half_width = int(min(half_width, sqrt(num_rows ** 2 + num_cols ** 2)))     # 반 너비가 이미지 내에 있는지 확인





        for row in range(-half_width, half_width + 1):

        range(-half_width, half_width + 1) 안에 있는지 확인합니다:

            for col in range(-half_width, half_width + 1):

            for col in range(-half_width, half_width + 1):

                row_rot = col * sin_angle + row * cos_angle

                row_rot = col * sin_angle + row * cos_angle

                col_rot = col * cos_angle - row * sin_angle

                col_rot = col * cos_angle - row * sin_angle

                row_bin = (row_rot / hist_width) + 0.5 * window_width - 0.5

                row_bin = (row_rot/hist_width) + 0.5 * window_width - 0.5

                col_bin = (col_rot / hist_width) + 0.5 * window_width - 0.5

                col_bin = (col_rot / hist_width) + 0.5 * window_width - 0.5

                if row_bin > -1 and row_bin < window_width and col_bin > -1 and col_bin < window_width:

                행_빈 > -1이고 행_빈 < window_width 및 col_bin > -1이고 col_bin < window_width인 경우:

                    window_row = int(round(point[1] + row))

                    window_row = int(round(point[1] + row))

                    window_col = int(round(point[0] + col))

                    window_col = int(round(point[0] + col))

                    if window_row > 0 and window_row < num_rows - 1 and window_col > 0 and window_col < num_cols - 1:

                    window_row > 0이고 window_row < num_rows - 1이고 window_col > 0이고 window_col < num_cols - 1인 경우:

                        dx = gaussian_image[window_row, window_col + 1] - gaussian_image[window_row, window_col - 1]

                        dx = 가우시안_이미지[window_row, window_col + 1] - 가우시안_이미지[window_row, window_col - 1]

                        dy = gaussian_image[window_row - 1, window_col] - gaussian_image[window_row + 1, window_col]

                        dy = 가우시안_이미지[window_row - 1, window_col] - 가우시안_이미지[window_row + 1, window_col]

                        gradient_magnitude = sqrt(dx * dx + dy * dy)

                        gradient_magnitude = sqrt(dx * dx + dy * dy)

                        gradient_orientation = rad2deg(arctan2(dy, dx)) % 360

                        그라데이션_방향 = rad2deg(arctan2(dy, dx)) % 360

                        weight = exp(weight_multiplier * ((row_rot / hist_width) ** 2 + (col_rot / hist_width) ** 2))

                        weight = exp(weight_multiplier * ((row_rot / hist_width) ** 2 + (col_rot / hist_width) ** 2))

                        row_bin_list.append(row_bin)

                        row_bin_list.append(row_bin)

                        col_bin_list.append(col_bin)

                        col_bin_list.append(col_bin)

                        magnitude_list.append(weight * gradient_magnitude)

                        magnitude_list.append(weight * gradient_magnitude)

                        orientation_bin_list.append((gradient_orientation - angle) * bins_per_degree)

                        orientation_bin_list.append((gradient_orientation - angle) * bins_per_degree)





        for row_bin, col_bin, magnitude, orientation_bin in zip(row_bin_list, col_bin_list, magnitude_list, orientation_bin_list):

        for row_bin, col_bin, magnitude, orientation_bin in zip(row_bin_list, col_bin_list, magnitude_list, orientation_bin_list):

            # Smoothing via trilinear interpolation

            # 삼선 보간을 통한 스무딩

            # Notations follows https://en.wikipedia.org/wiki/Trilinear_interpolation

            # 표기법은 https://en.wikipedia.org/wiki/Trilinear_interpolation 을 따릅니다.

            # Note that we are really doing the inverse of trilinear interpolation here (we take the center value of the cube and distribute it among its eight neighbors)

            # 여기서 실제로는 삼선 보간을 역으로 하고 있다는 점에 유의하세요(큐브의 중심값을 취해 8개의 이웃에 분배합니다).

            row_bin_floor, col_bin_floor, orientation_bin_floor = floor([row_bin, col_bin, orientation_bin]).astype(int)

            row_bin_floor, col_bin_floor, orientation_bin_floor = floor([row_bin, col_bin, orientation_bin]).astype(int)

            row_fraction, col_fraction, orientation_fraction = row_bin - row_bin_floor, col_bin - col_bin_floor, orientation_bin - orientation_bin_floor

            row_fraction, col_fraction, orientation_fraction = row_bin - row_bin_floor, col_bin - col_bin_floor, orientation_bin - orientation_bin_floor

            if orientation_bin_floor < 0:

            오리엔테이션_빈_바닥이 0이면

                orientation_bin_floor += num_bins

                오리엔테이션_빈_바닥 += NUM_BINS

            if orientation_bin_floor >= num_bins:

            if orientation_bin_floor >= num_bins:

                orientation_bin_floor -= num_bins

                오리엔테이션_빈_바닥 -= NUM_BINS





            c1 = magnitude * row_fraction

            c1 = magnitude * row_fraction

            c0 = magnitude * (1 - row_fraction)

            c0 = magnitude * (1 - row_fraction)

            c11 = c1 * col_fraction

            C11 = C1 * COL_FRACTION

            c10 = c1 * (1 - col_fraction)

            C10 = C1 * (1 - COL_FRACTION)

            c01 = c0 * col_fraction

            C01 = C0 * COL_FRACTION

            c00 = c0 * (1 - col_fraction)

            C00 = C0 * (1 - COL_FRACTION)

            c111 = c11 * orientation_fraction

            C111 = C11 * 오리엔테이션_프랙션

            c110 = c11 * (1 - orientation_fraction)

            C110 = C11 * (1 - 오리엔테이션_프랙션)

            c101 = c10 * orientation_fraction

            C101 = C10 * 오리엔테이션_프랙션

            c100 = c10 * (1 - orientation_fraction)

            C100 = C10 * (1 - 오리엔테이션_프랙션)

            c011 = c01 * orientation_fraction

            C011 = C01 * 오리엔테이션_프랙션

            c010 = c01 * (1 - orientation_fraction)

            C010 = C01 * (1 - 오리엔테이션_프랙션)

            c001 = c00 * orientation_fraction

            C001 = C00 * 오리엔테이션_프랙션

            c000 = c00 * (1 - orientation_fraction)

            C000 = C00 * (1 - 오리엔테이션_프랙션)





            histogram_tensor[row_bin_floor + 1, col_bin_floor + 1, orientation_bin_floor] += c000

            히스토그램_텐서[행_빈_바닥 + 1, col_빈_바닥 + 1, 오리엔테이션_빈_바닥] += c000

            histogram_tensor[row_bin_floor + 1, col_bin_floor + 1, (orientation_bin_floor + 1) % num_bins] += c001

            히스토그램_텐서[row_bin_바닥 + 1, col_bin_바닥 + 1, (orientation_bin_바닥 + 1) % num_bins] += c001

            histogram_tensor[row_bin_floor + 1, col_bin_floor + 2, orientation_bin_floor] += c010

            히스토그램_텐서[row_bin_바닥 + 1, col_bin_바닥 + 2, orientation_bin_바닥] += c010

            histogram_tensor[row_bin_floor + 1, col_bin_floor + 2, (orientation_bin_floor + 1) % num_bins] += c011

            히스토그램_텐서[row_bin_바닥 + 1, col_bin_바닥 + 2, (orientation_bin_바닥 + 1) % num_bins] += c011

            histogram_tensor[row_bin_floor + 2, col_bin_floor + 1, orientation_bin_floor] += c100

            히스토그램_텐서[row_bin_바닥 + 2, col_bin_바닥 + 1, orientation_bin_바닥] += c100

            histogram_tensor[row_bin_floor + 2, col_bin_floor + 1, (orientation_bin_floor + 1) % num_bins] += c101

            히스토그램_텐서[row_bin_바닥 + 2, col_bin_바닥 + 1, (orientation_bin_바닥 + 1) % num_bins] += c101

            histogram_tensor[row_bin_floor + 2, col_bin_floor + 2, orientation_bin_floor] += c110

            히스토그램_텐서[row_bin_바닥 + 2, col_bin_바닥 + 2, orientation_bin_바닥] += c110

            histogram_tensor[row_bin_floor + 2, col_bin_floor + 2, (orientation_bin_floor + 1) % num_bins] += c111

            히스토그램_텐서[row_bin_바닥 + 2, col_bin_바닥 + 2, (orientation_bin_바닥 + 1) % num_bins] += c111





        descriptor_vector = histogram_tensor[1:-1, 1:-1, :].flatten()  # Remove histogram borders

        descriptor_vector = histogram_tensor[1:-1, 1:-1, :].flatten() # 히스토그램 테두리를 제거합니다.

        # Threshold and normalize descriptor_vector

        # 설명자_벡터의 임계값을 설정하고 정규화합니다.

        threshold = norm(descriptor_vector) * descriptor_max_value

        threshold = norm(descriptor_vector) * descriptor_max_value

        descriptor_vector[descriptor_vector > threshold] = threshold

        descriptor_vector[descriptor_vector > 임계값] = 임계값

        descriptor_vector /= max(norm(descriptor_vector), float_tolerance)

        descriptor_vector /= max(norm(descriptor_vector), float_tolerance)

        # Multiply by 512, round, and saturate between 0 and 255 to convert from float32 to unsigned char (OpenCV convention)

        # 512를 곱하고, 반올림하고, 0에서 255 사이로 포화시켜 float32에서 부호 없는 문자로 변환합니다(OpenCV 규칙).

        descriptor_vector = round(512 * descriptor_vector)

        descriptor_vector = round(512 * descriptor_vector)

        descriptor_vector[descriptor_vector < 0] = 0

        descriptor_vector[descriptor_vector < 0] = 0

        descriptor_vector[descriptor_vector > 255] = 255

        descriptor_vector[descriptor_vector > 255] = 255

        descriptors.append(descriptor_vector)

        descriptors.append(descriptor_vector)

    return array(descriptors, dtype='float32')

    return array(descriptors, dtype='float32')

```

```





At last, descriptor generation. Descriptors encode information about a keypoint’s neighborhood and allow comparison between keypoints. SIFT descriptors are particularly well designed, enabling robust keypoint matching.

드디어 디스크립터 생성입니다. 디스크립터는 키포인트의 이웃에 대한 정보를 인코딩하고 키포인트 간의 비교를 가능하게 합니다. SIFT 디스크립터는 특히 잘 설계되어 강력한 키포인트 매칭을 가능하게 합니다.





For each keypoint, our first step is to create another histogram of gradient orientations. We consider a square neighborhood (different side length this time) around each keypoint, but now we rotate this neighborhood by the keypoint’s angle. This is what makes SIFT invariant to rotation. From this rotated neighborhood, we do something special. We compute row and column bins, which are just indices local to the neighborhood that denote where each pixel lies. We also calculate each pixel’s gradient magnitude and orientation, like we did when computing keypoint orientations. However, instead of actually building a histogram and accumulating values, we simply store the histogram bin index and bin value for each pixel. Note that here our histograms have only 8 bins to cover 360 degrees, instead of 36 bins as before.

각 키포인트에 대해 첫 번째 단계는 그라데이션 방향의 또 다른 히스토그램을 만드는 것입니다. 각 키포인트 주위에 정사각형 이웃(이번에는 다른 변 길이)을 고려하지만, 이제 이 이웃을 키포인트의 각도에 따라 회전시킵니다. 이것이 바로 SIFT가 회전에 불변하는 이유입니다. 이 회전된 이웃에서 특별한 작업을 수행합니다. 각 픽셀의 위치를 나타내는 이웃에 국한된 인덱스인 행과 열 구간차원을 계산합니다. 또한 키포인트 방향을 계산할 때와 마찬가지로 각 픽셀의 그라데이션 크기와 방향도 계산합니다. 하지만 실제로 히스토그램을 만들고 값을 누적하는 대신 각 픽셀에 대한 히스토그램 구간차원 인덱스와 구간차원 값을 저장하기만 하면 됩니다. 여기서 히스토그램은 이전처럼 36개의 빈이 아니라 360도를 커버할 수 있는 8개의 빈만 있다는 점에 유의하세요.





Now comes the tricky part. Imagine that we take our square neighborhood, a 2D array, and replace each pixel with a vector of length 36. The orientation bin associated with each pixel will index into its 36-length vector, and at this location we’ll store the weighted gradient magnitude for this pixel. This will form a 3D array of size `(window_width, window_width, num_bins)`, which evaluates to `(4, 4, 36)`. We’re going to flatten this 3D array to serve as our descriptor vector. Before that, however, it’s a good idea to apply some smoothing.

이제 까다로운 부분이 나옵니다. 2D 배열인 정사각형 이웃을 가지고 각 픽셀을 길이 36의 벡터로 대체한다고 상상해 보겠습니다. 각 픽셀과 관련된 방향 구간은 36 길이 벡터로 인덱싱되고, 이 위치에 이 픽셀에 대한 가중 그라데이션 크기를 저장합니다. 이렇게 하면 `(4, 4, 36)`으로 평가되는 크기 `(window_width, window_width, num_bins)`의 3D 배열이 형성됩니다. 이 3D 배열을 평평하게 만들어 설명자 벡터로 사용하겠습니다. 하지만 그 전에 약간의 평활화를 적용하는 것이 좋습니다.





What kind of smoothing? Hint: It involves more finite differences. We’ll smooth the weighted gradient magnitude for each neighborhood pixel by distributing it among its eight neighbors in three dimensions: row bin, column bin, and orientation bin. We’ll use a method known as trilinear interpolation, or more precisely, we’ll use its inverse. Wikipedia provides the formulas and a good visualization of the method. Put simply, each neighborhood pixel has a row bin index, column bin index, and orientation bin index, and we want to distribute its histogram value proportionately to its eight neighbor bins, all while making sure the distributed parts add up to the original value.

어떤 종류의 스무딩일까요? 힌트: 더 미세한 차이를 포함합니다. 각 이웃 픽셀의 가중 그라데이션 크기를 행 구간, 열 구간, 방향 구간 등 3차원의 8개의 이웃 픽셀에 분배하여 각 픽셀의 가중 그라데이션 크기를 평활화할 것입니다. 삼선 보간이라는 방법을 사용하거나 더 정확하게는 그 역을 사용할 것입니다. 위키백과에 이 방법에 대한 공식과 시각화가 잘 나와 있습니다. 간단히 말해, 각 이웃 픽셀에는 행 구간차원 인덱스, 열 구간차원 인덱스 및 방향 구간차원 인덱스가 있으며, 히스토그램 값을 8개의 이웃 구간차원에 비례적으로 분배하면서 분배된 부분이 원래 값에 합산되도록 하려는 것입니다.





You may wonder why we don’t simply allocate one-eighth of the histogram value to each of the eight neighbors. The problem is the fact that the neighborhood pixel may have fractional bin indices. Imagine each neighborhood pixel is represented by the 3D point `[row_bin, col_bin, orientation_bin]`, shown by the red point in the figure below. This 3D point may not lie at the exact center of the cube formed by its eight integer-value neighbor points, the blue points below. If we want to accurately distribute the value at the red point to the blue points, the blue points that are closer to the red point should receive a larger proportion of the red point’s value. This is precisely what (the inverse of) trilinear interpolation does. We split the red point into two green points, we split the two green points into four points, and finally we split the four points into our final eight points.

왜 단순히 히스토그램 값의 8분의 1을 8개의 이웃 빈에 각각 할당하지 않는지 궁금할 수 있습니다. 문제는 이웃 픽셀에 분수 구간차원 인덱스가 있을 수 있다는 사실입니다. 아래 그림에서 빨간색 점으로 표시된 것처럼 각 이웃 픽셀이 3D 점 `[row_bin, col_bin, orientation_bin]`으로 표시된다고 상상해 보십시오. 이 3D 점은 8개의 정수 값 이웃 점인 아래 파란색 점으로 구성된 정육면체의 정확한 중심에 있지 않을 수 있습니다. 빨간색 점의 값을 파란색 점들에 정확하게 분배하려면 빨간색 점에 더 가까운 파란색 점들이 빨간색 점의 값의 더 큰 비율을 받아야 합니다. 이것이 바로 (역의) 삼선 보간이 하는 일입니다. 빨간색 점을 두 개의 녹색 점으로 나누고, 두 개의 녹색 점을 네 개의 점으로 나누고, 마지막으로 네 개의 점을 최종 8개의 점으로 나눕니다.









<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_06.png" width="300px"></p>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_06.png" width="300px"></p>





<center>We perform the inverse of trilinear interpolation, taking the possibly off-center red point and distributing it among its eight neighbors (image from Wikipedia).</center>

<center>삼선 보간의 역을 수행하여 중심에서 벗어날 가능성이 있는 빨간색 점을 취하여 8개의 이웃에 분배합니다(위키백과 이미지).</center>





<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_07.png" width="300px"></p>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_07.png" width="300px"></p>





<center>We first divide the red point into two possibly unequal green points, and recurse until we reach the blue points (image from Wikipedia).</center>

<center>우리는 먼저 빨간색 점을 두 개의 같지 않은 녹색 점으로 나누고 파란색 점에 도달할 때까지 재귀합니다(Wikipedia의 이미지).</center>





Our last step is to flatten our smoothed 3D array into a descriptor vector of length 128. Then we’ll apply a threshold and normalize. In the OpenCV implementation, the descriptor is then scaled and saturated to lie between 0 and 255 for efficiency when comparing descriptors later.

마지막 단계는 평활화된 3D 배열을 길이 128의 설명자 벡터로 평평하게 만드는 것입니다. 그런 다음 임계값을 적용하고 정규화합니다. 그런 다음 OpenCV 구현에서는 나중에 설명자를 비교할 때 효율성을 위해 설명자의 크기를 조정하고 0에서 255 사이에 위치하도록 포화시킵니다.





We repeat this procedure to generate one descriptor vector for each keypoint.

이 절차를 반복하여 각 키포인트에 대해 하나의 디스크립터 벡터를 생성합니다.





That’s the entire SIFT algorithm. I know it was a lot, but you’ve made it this far! Now let’s see SIFT in action with a template matching demo.

이것이 전체 SIFT 알고리즘입니다. 많이 어렵겠지만 여기까지 오셨습니다! 이제 템플릿 매칭 데모를 통해 SIFT가 실제로 어떻게 작동하는지 살펴보겠습니다.





# Application: Template Matching

# 애플리케이션: 템플릿 매칭

```

```

import numpy as np

import numpy as np

import cv2

import cv2

import pysift

import pysift

from matplotlib import pyplot as plt

matplotlib에서 pyplot을 plt로 가져옵니다.

import logging

logging 임포트

logger = logging.getLogger(__name__)

logger = logging.getLogger(__name__)





MIN_MATCH_COUNT = 10

min_match_count = 10





img1 = cv2.imread('box.png', 0)           # queryImage

img1 = cv2.imread('box.png', 0) # queryImage

img2 = cv2.imread('box_in_scene.png', 0)  # trainImage

img2 = cv2.imread('box_in_scene.png', 0) # trainImage





# Compute SIFT keypoints and descriptors

# SIFT 키포인트와 설명자 계산하기

kp1, des1 = pysift.computeKeypointsAndDescriptors(img1)

kp1, des1 = pysift.computeKeypointsAndDescriptors(img1)

kp2, des2 = pysift.computeKeypointsAndDescriptors(img2)

kp2, des2 = pysift.computeKeypointsAndDescriptors(img2)





# Initialize and use FLANN

# FLANN 초기화 및 사용

FLANN_INDEX_KDTREE = 0

flann_index_kdtree = 0

index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)

index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)

search_params = dict(checks = 50)

search_params = dict(checks = 50)

flann = cv2.FlannBasedMatcher(index_params, search_params)

flann = cv2.FlannBasedMatcher(index_params, search_params)

matches = flann.knnMatch(des1, des2, k=2)

matches = flann.knnMatch(des1, des2, k=2)





# Lowe's ratio test

# 로우 비율 테스트

good = []

good = []

for m, n in matches:

일치 항목의 m, n에 대해

    if m.distance < 0.7 * n.distance:

    if m.distance < 0.7 * n.distance:

        good.append(m)

        good.append(m)





if len(good) > MIN_MATCH_COUNT:

len(good) > MIN_MATCH_COUNT:

    # Estimate homography between template and scene

    # 템플릿과 장면 사이의 동형성 추정

    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)

    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)

    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)





    M = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)[0]

    M = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)[0]





    # Draw detected template in scene image

    # 장면 이미지에 감지된 템플릿 그리기

    h, w = img1.shape

    h, w = img1.shape

    pts = np.float32([[0, 0],

    pts = np.float32([[0, 0],

                      [0, h - 1],

                      [0, h - 1],

                      [w - 1, h - 1],

                      [w - 1, h - 1],

                      [w - 1, 0]]).reshape(-1, 1, 2)

                      [w - 1, 0]]).reshape(-1, 1, 2)

    dst = cv2.perspectiveTransform(pts, M)

    dst = cv2.perspectiveTransform(pts, M)





    img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)

    img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)





    h1, w1 = img1.shape

    h1, w1 = img1.shape

    h2, w2 = img2.shape

    h2, w2 = img2.shape

    nWidth = w1 + w2

    n폭 = w1 + w2

    nHeight = max(h1, h2)

    n높이 = 최대(h1, h2)

    hdif = int((h2 - h1) / 2)

    hdif = int((h2 - h1) / 2)

    newimg = np.zeros((nHeight, nWidth, 3), np.uint8)

    newimg = np.zeros((nHeight, nWidth, 3), np.uint8)





    for i in range(3):

    범위(3)의 i에 대해

        newimg[hdif:hdif + h1, :w1, i] = img1

        newimg[hdif:hdif + h1, :w1, i] = img1

        newimg[:h2, w1:w1 + w2, i] = img2

        newimg[:h2, w1:w1 + w2, i] = img2





    # Draw SIFT keypoint matches

    # SIFT 키포인트 일치 항목 그리기

    for m in good:

    를 그립니다:

        pt1 = (int(kp1[m.queryIdx].pt[0]), int(kp1[m.queryIdx].pt[1] + hdif))

        pt1 = (int(kp1[m.queryIdx].pt[0]), int(kp1[m.queryIdx].pt[1] + hdif))

        pt2 = (int(kp2[m.trainIdx].pt[0] + w1), int(kp2[m.trainIdx].pt[1]))

        pt2 = (int(kp2[m.trainIdx].pt[0] + w1), int(kp2[m.trainIdx].pt[1]))

        cv2.line(newimg, pt1, pt2, (255, 0, 0))

        cv2.line(newimg, pt1, pt2, (255, 0, 0))





    plt.imshow(newimg)

    plt.imshow(newimg)

    plt.show()

    plt.show()

else:

else:

    print("Not enough matches are found - %d/%d" % (len(good), MIN_MATCH_COUNT))

    print("일치하는 항목이 충분하지 않습니다 - %d/%d" % (len(good), MIN_MATCH_COUNT))

```

```

This script from the repo is adapted from OpenCV’s template matching demo. Given a template image, the goal is to detect it in a scene image and compute the homography. We compute SIFT keypoints and descriptors on both the template image and the scene image, and perform approximate nearest neighbors search on the two sets of descriptors to find similar keypoints. Keypoint pairs closer than a threshold are considered good matches. Finally, we perform RANSAC on the keypoint matches to compute the best fit homography.

리포지토리의 이 스크립트는 OpenCV의 템플릿 매칭 데모를 각색한 것입니다. 템플릿 이미지가 주어지면 장면 이미지에서 이를 감지하고 동형성을 계산하는 것이 목표입니다. 템플릿 이미지와 장면 이미지 모두에서 SIFT 키포인트와 설명자를 계산하고, 두 설명자 세트에서 근사 근접 이웃 검색을 수행하여 유사한 키포인트를 찾습니다. 임계값보다 가까운 키포인트 쌍은 잘 일치하는 것으로 간주됩니다. 마지막으로, 일치하는 키포인트에 대해 RANSAC을 수행하여 가장 적합한 호모그래피를 계산합니다.





Give it a try. You should see something like this:

한 번 해보세요. 다음과 같은 결과가 나올 것입니다:





<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_08.png" width="600px"></p>

<p align="center"><img src="https://wikidocs.net/images/page/228065/Fig_SIFT_08.png" width="600px"></p>





<center>Output plot of template_matching_demo.py, showing the detected template along with keypoint matches.</center>

<center>키포인트 일치 항목과 함께 감지된 템플릿을 보여주는 template_matching_demo.py의 출력 플롯</center>





Try this script with your own template and scene images to get a feel for the stability of SIFT keypoints. You can explore under which conditions they work well, and under which conditions they perform poorly.

자체 템플릿과 장면 이미지로 이 스크립트를 사용해 SIFT 키포인트의 안정성을 느껴보세요. 어떤 조건에서 잘 작동하고 어떤 조건에서 성능이 저하되는지 살펴볼 수 있습니다.





# Wrapping Up

# 마무리





You’ve made it! SIFT is an indispensable component of any computer vision engineer’s toolkit. I hope this tutorial has shed light on the myriad implementation details that elevate SIFT from just a nice idea on paper to practical, robust code. Don’t forget to try SIFT on your future vision projects, whether you use it on its own, as part of a larger image processing pipeline, or just as a benchmark against learning-based approaches.

다 끝났습니다! SIFT는 모든 컴퓨터 비전 엔지니어의 툴킷에 없어서는 안 될 구성 요소입니다. 이 튜토리얼을 통해 SIFT를 종이 위의 멋진 아이디어에서 실용적이고 강력한 코드로 끌어올리는 무수한 구현 세부 사항을 살펴볼 수 있었기를 바랍니다. 단독으로 사용하든, 더 큰 이미지 처리 파이프라인의 일부로 사용하든, 학습 기반 접근 방식에 대한 벤치마크로 사용하든 향후 비전 프로젝트에 SIFT를 사용해 보는 것을 잊지 마세요.

